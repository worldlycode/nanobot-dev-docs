{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NanoBot-Dev Documentation \ud83e\udd16","text":"<p>Welcome to the NanoBot-Dev documentation. This site provides comprehensive information about using and extending the NanoBot development pipeline.</p>"},{"location":"#what-is-nanobot","title":"What is NanoBot?","text":"<p>NanoBot is a GenAI powered document processing and retrieval system with vector search capabilities. It allows you to:</p> <ul> <li>Process documents (PDF, DOCX, TXT, Google Sheets etc)</li> <li>Chunk documents into manageable pieces</li> <li>Generate vector embeddings</li> <li>Store the document data and text in a relational database</li> <li>Search documents via a simple chatbot interface</li> </ul> <p></p>"},{"location":"#purpose-of-this-development-codebase","title":"Purpose of this development codebase","text":"<p>We are developing a codebase that creates an intuitive pipeline that can be easily followed, understood and modified by all members involved in the development. The Dev component is a robust pipeline that can empower people and organizations to generate their very own Bot with their own documentation, enabling them to also demonstrate and understand the power and potential of GenAI in their own labs, facilities and research.</p> <p>This project presumes that an out-of-the-box chatbot developed using many online WYSIWYG frameworks is great for concept, but often fails on utility. By introducing and teaching these concepts in a python codebase framework, we open up the power of Generative AI with only a few customizations.  </p> <p>It is our hope by doing so, that we not only create a cohesive codebase with functional code for our respective facilities, but that we are able to bring new members on board to help them employ instances of this codebase at their own locations. This building of code simultaneously builds community and further collaboration \u2014 this is \\(\\mathbf{C}^3\\)</p>"},{"location":"#our-current-selections-for-infrastructure","title":"Our Current Selections for Infrastructure","text":"<p>As of 4/20/2025</p> <p>The current codebase aims to simplify some of the initial setup and modification of the basic steps of an application.  We have some boilerplate code that organized the steps of a RAG (Retrieval Augmented Generation) Generative AI system, and that has consolidated areas of code where one can modify the parameters of each module, simplyfying the use of the codebase.</p>"},{"location":"#python-as-our-language","title":"Python as our Language","text":"<p>Pyuthon is the standard language for most Generative AI applications, and is particullary easy to use. This module require <code>Python 3.12</code> or above.  </p>"},{"location":"#document-parsing-and-chunking","title":"Document parsing and chunking","text":"<p>We are currently using IBM's Docling which can be used to convert most general document formats (and even websites) into a format readable and usable to most Generative AI models.  In particular it is especially good at PDFs, and creates a Docling Document that seperatly identifies and extracts text, tables, pictures, document hierarchy with headers, sections and groups, and understands the content within the context of the page.  </p> <p>To start, we are extracting all the text and table and the provenance information (such as pages number, file name, and heading the information is part of)</p>"},{"location":"#database","title":"Database","text":"<p>We currently are using Neon.tech for our Postgres database.  Postgres is an optimal solution at this early stage as postgres allows, via the pgvector plugin, semantic similarity searches by vector arithmetic.  </p> <p>This choice to use Neon.Tech provides a web based solution, using the power of postgres and takes advantage of Neon's capability for data versioning and branching.</p>"},{"location":"#prompts-with-jinja-templates","title":"Prompts with Jinja Templates","text":"<p>In this structure, we separate out the prompts for each call to the GenAI model so as to extract this from the working code, to make it more transparent, and to be able to easily modify the prompt for testing.</p> <p>The prompts are formatted and delivered to the model using Jinja templates, which also allows us to separate the functionality from the other Python code as well.  </p>"},{"location":"#choice-of-generative-ai-models","title":"Choice of Generative AI Models","text":"<p>At this point we are using OpenAI for all aspects of our GenAI needs.  This includes using <code>4o</code> for the cleaning and formatting of the documents, the embedding of the vectors using , and <code>4o</code> for the chat completion steps.  </p> <p>This choice is currently determined by the quality and output of the model, and the ease of interacting with the API.  </p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Getting Started - Installation and basic setup</li> <li>User Guide - Detailed usage instructions</li> <li>API Reference - Technical reference for developers</li> <li>Examples - Code examples and tutorials</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>this is where examples will go</p>"},{"location":"api/database/","title":"Database","text":""},{"location":"api/database/#understanding-the-database-transaction-management-code","title":"Understanding the Database Transaction Management Code","text":"<p>This code is in <code>/database/transaction.py</code> but is not yet implemented except in the <code>/tests/</code> directory</p> <p>Let's break down this transaction management code step by step:</p>"},{"location":"api/database/#1-imports-and-setup","title":"1. Imports and Setup","text":"<pre><code>import time\nimport logging\nimport psycopg2\nfrom contextlib import contextmanager\nfrom typing import Optional, Callable, Any\nfrom app.database.db_common import get_connection\nfrom app.config.settings import settings\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n</code></pre> <p>This section: - Imports necessary libraries (time for measuring duration, logging for feedback) - Sets up a logger for this module</p>"},{"location":"api/database/#2-custom-exception-classes","title":"2. Custom Exception Classes","text":"<pre><code>class TransactionError(Exception):\n    \"\"\"Exception raised for transaction-related errors.\"\"\"\n    pass\n\nclass TransactionTimeout(TransactionError):\n    \"\"\"Exception raised when a transaction times out.\"\"\"\n    pass\n</code></pre> <p>These are custom exception classes: - <code>TransactionError</code>: A general error for any transaction problems - <code>TransactionTimeout</code>: A specific error for when transactions take too long (inherits from TransactionError)</p>"},{"location":"api/database/#3-the-main-transaction-context-manager","title":"3. The Main Transaction Context Manager","text":"<pre><code>@contextmanager\ndef transaction(\n    use_neon: Optional[bool] = None,\n    timeout: Optional[float] = None,\n    feedback: bool = True,\n    auto_rollback: bool = True\n):\n</code></pre> <p>This is a context manager (used with <code>with</code> statements) that: - Takes parameters to control behavior:   - <code>use_neon</code>: Whether to use Neon database (overrides settings)   - <code>timeout</code>: Maximum seconds to allow for the transaction   - <code>feedback</code>: Whether to log what's happening   - <code>auto_rollback</code>: Whether to automatically rollback on errors</p>"},{"location":"api/database/#4-transaction-setup","title":"4. Transaction Setup","text":"<pre><code>conn = None\nstart_time = time.time()\ndb_type = \"Neon\" if (use_neon if use_neon is not None else settings.use_neon) else \"local\"\n\ntry:\n    # Get database connection\n    if feedback:\n        logger.info(f\"Starting transaction with {db_type} database\")\n\n    conn = get_connection(use_neon=use_neon)\n</code></pre> <p>This part: - Initializes variables (connection and start time) - Determines which database to use - Logs the start of the transaction (if feedback is enabled) - Gets a database connection</p>"},{"location":"api/database/#5-setting-timeout","title":"5. Setting Timeout","text":"<pre><code># Set timeout if specified\nif timeout is not None and timeout &gt; 0:\n    # Set statement timeout in milliseconds\n    with conn.cursor() as cur:\n        cur.execute(f\"SET statement_timeout = {int(timeout * 1000)}\")\n</code></pre> <p>This sets a PostgreSQL statement timeout if one was specified: - Converts seconds to milliseconds - Tells PostgreSQL to abort any statement that takes longer than this time</p>"},{"location":"api/database/#6-yielding-the-connection","title":"6. Yielding the Connection","text":"<pre><code># Yield connection to the caller\nyield conn\n</code></pre> <p>This is where the context manager pauses and gives control to the code inside the <code>with</code> block. - The connection is passed to your code to use - When your code finishes, execution continues after this line</p>"},{"location":"api/database/#7-committing-the-transaction","title":"7. Committing the Transaction","text":"<pre><code># Check timeout before committing\nif timeout is not None and (time.time() - start_time) &gt; timeout:\n    raise TransactionTimeout(f\"Transaction timed out after {timeout} seconds\")\n\n# Commit the transaction\nconn.commit()\n\nif feedback:\n    duration = time.time() - start_time\n    logger.info(f\"Transaction committed successfully ({duration:.2f}s)\")\n</code></pre> <p>After your code finishes: - It checks if we've exceeded the timeout - If not, it commits the transaction (saves changes to the database) - Logs the successful commit with duration</p>"},{"location":"api/database/#8-handling-connection-errors","title":"8. Handling Connection Errors","text":"<pre><code>except psycopg2.OperationalError as e:\n    if \"statement timeout\" in str(e).lower():\n        if feedback:\n            logger.error(f\"Transaction timed out after {timeout} seconds\")\n        raise TransactionTimeout(f\"Database operation timed out after {timeout} seconds\") from e\n\n    if feedback:\n        logger.error(f\"Database connection error: {e}\")\n\n    if conn and auto_rollback:\n        try:\n            conn.rollback()\n            if feedback:\n                logger.info(\"Transaction rolled back due to connection error\")\n        except Exception as rollback_error:\n            if feedback:\n                logger.error(f\"Failed to rollback transaction: {rollback_error}\")\n\n    raise TransactionError(f\"Database connection error: {e}\") from e\n</code></pre> <p>This handles database connection errors: - Checks if it's a timeout error - Logs the error - Rolls back the transaction if auto_rollback is enabled - Raises a custom exception</p>"},{"location":"api/database/#9-handling-other-exceptions","title":"9. Handling Other Exceptions","text":"<pre><code>except Exception as e:\n    duration = time.time() - start_time\n\n    if feedback:\n        logger.error(f\"Transaction failed after {duration:.2f}s: {e}\")\n\n    if conn and auto_rollback:\n        try:\n            conn.rollback()\n            if feedback:\n                logger.info(\"Transaction rolled back due to error\")\n        except Exception as rollback_error:\n            if feedback:\n                logger.error(f\"Failed to rollback transaction: {rollback_error}\")\n\n    # Re-raise the original exception\n    raise\n</code></pre> <p>This handles any other exceptions: - Logs the error with duration - Rolls back the transaction if auto_rollback is enabled - Re-raises the original exception</p>"},{"location":"api/database/#10-cleanup","title":"10. Cleanup","text":"<pre><code>finally:\n    # Close connection in finally block to ensure it happens\n    if conn:\n        try:\n            conn.close()\n            if feedback:\n                logger.debug(\"Database connection closed\")\n        except Exception as close_error:\n            if feedback:\n                logger.error(f\"Error closing database connection: {close_error}\")\n</code></pre> <p>This cleanup always runs: - Closes the database connection - Logs any errors that occur during closing</p>"},{"location":"api/database/#11-helper-function","title":"11. Helper Function","text":"<pre><code>def execute_with_transaction(\n    func: Callable,\n    *args,\n    use_neon: Optional[bool] = None,\n    timeout: Optional[float] = None,\n    feedback: bool = True,\n    auto_rollback: bool = True,\n    **kwargs\n) -&gt; Any:\n</code></pre> <p>This is a helper function that: - Takes a function and its arguments - Takes the same transaction parameters - Executes the function within a transaction</p> <pre><code>with transaction(\n    use_neon=use_neon,\n    timeout=timeout,\n    feedback=feedback,\n    auto_rollback=auto_rollback\n) as conn:\n    return func(conn, *args, **kwargs)\n</code></pre> <p>It: - Creates a transaction using the context manager - Calls your function with the connection and other arguments - Returns whatever your function returns</p> <p>This helper makes it easier to use transactions without writing the <code>with</code> statement every time.</p>"},{"location":"api/database/#12-usage-example","title":"12 Usage Example","text":"<pre><code>from app.database.transaction import transaction, execute_with_transaction, TransactionError, TransactionTimeout\n\ndef insert_chunk(conn, text, vector, metadata):\n    \"\"\"Insert a chunk into the database.\"\"\"\n    # ... existing implementation ...\n    return chunk_id\n\ndef insert_chunk_with_transaction(text, vector, metadata, use_neon=None, timeout=None):\n    \"\"\"Insert a chunk with transaction management.\"\"\"\n    try:\n        with transaction(use_neon=use_neon, timeout=timeout) as conn:\n            chunk_id = insert_chunk(conn, text, vector, metadata)\n            return chunk_id\n    except TransactionTimeout:\n        print(\"Insert operation timed out\")\n        return None\n    except TransactionError as e:\n        print(f\"Transaction error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n\n# Alternative using the execute_with_transaction helper\ndef insert_chunk_with_helper(text, vector, metadata, use_neon=None, timeout=None):\n    \"\"\"Insert a chunk using the transaction helper.\"\"\"\n    return execute_with_transaction(\n        insert_chunk,\n        text, vector, metadata,\n        use_neon=use_neon,\n        timeout=timeout\n    )\n</code></pre>"},{"location":"api/services/","title":"Services","text":""},{"location":"assets/templates/blog-template/","title":"[Your Post Title Here]","text":"<p>To copy this template: <pre><code>   cp docs/blog/blog-template.md docs/blog/YYYY-MM-DD-title.md\n</code></pre></p> <p>Date: [Month Day, Year] Author: Worldly Woman</p> <pre><code>\"hello world \ud83c\udf10\"</code></pre> <p>[Write your opening paragraph here. Introduce the topic and why it matters.]</p>"},{"location":"assets/templates/blog-template/#main-section-title","title":"[Main Section Title]","text":""},{"location":"assets/templates/blog-template/#subsection-title","title":"[Subsection Title]","text":"<p>[Main content goes here. You can include links like this: Link Text]</p> <p>You can include images like this:   </p> <p></p> <p>or like this:</p> <p>NOTE: [Use blockquotes for important notes or callouts]</p>"},{"location":"assets/templates/blog-template/#another-subsection","title":"[Another Subsection]","text":"<p>[More content here. Feel free to include code samples:]</p> <pre><code># Example code\ndef hello_world():\n    print(\"Hello, documentation world!\")\n</code></pre>"},{"location":"assets/templates/blog-template/#another-main-section","title":"[Another Main Section]","text":"<p>[Continue with more content as needed]</p>"},{"location":"assets/templates/blog-template/#technical-details","title":"[Technical Details]","text":"<p>[If applicable, include technical details, steps, or instructions]</p> <pre><code># Example command\ncommand --option value\n</code></pre>"},{"location":"assets/templates/blog-template/#detailed-subsection","title":"[Detailed Subsection]","text":"<p>[Add more detailed information if necessary]</p>"},{"location":"assets/templates/blog-template/#important-notes","title":"Important Notes","text":"<ul> <li>[First important point]  </li> <li>[Second important point]  </li> <li>[Third important point]</li> </ul>"},{"location":"assets/templates/blog-template/#closing-thoughts","title":"Closing Thoughts","text":"<p>[Summarize what you've covered and include any final thoughts or future directions]</p> <p>[Optional: Add a fun closing remark or personal touch]</p>"},{"location":"blog/00-index-blog/","title":"Hello World \ud83c\udf10","text":"<p>Welcome to the NanoBot-Dev development blog! </p> <p>Here our team will share updates, insights, and lessons learned while building this project.  </p> <p>Our motto:  Teach it to keep it!</p> <p>Click through the topics in the left sidebar to learn more.  </p>"},{"location":"blog/docling/","title":"Understanding IBM's Docling","text":"<p>Date: April 25, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>I am trying to understand the Docling Package, as it is powerful but somewhat complicated under the hood.  The first thing I am beginning to understand is that Docling is the open source itteration of a different tool it released in 2022, called the Deep Search for Discovery Toolkit (DS4SD).  Seep Search was the original solution for PDF documents, and now in 2024 they released docling in the Arxiv paper found at this link.  </p>"},{"location":"blog/docling/#pdf-conversions","title":"PDF Conversions","text":"<p>This is perhaps the main reason to use Docling.  Most python package PDF converters handle the text of a PDF document, and possibly even try to identify tables and images. But the latter in my experience they do not do well.  In addition to that the traditional packages do not capture the context, so they are not able to handle the chunking of \"Sections\" properly.  This means they may not know that a paragraph belongs to a particular section, and perhaps should not be spilt etc.  </p> <p>In any event, these are thisngs that Docling does very well, expecially for something that runs locally and free of charge.  (There are models such as Unstructures, Azure Intellisense, etc that also do a reasonable job but cost money).  Lastly there are methods that are rather state of the art that use MMLM with vision capabilities, to take in each PDF page as an image and then using these models to identify the areas and components of each page.  </p> <p>However, with Docling, IBM has trained two models that run locally (either on CPU or with GPU assist) These models are found on Hugging Face and include:</p> <ul> <li> <p>Layout Model: The layout model will take an image from a page and apply RT-DETR model (RT-DETR is an object detection model that stands for \u201cReal-Time DEtection Transformer.\u201d) in order to find different layout components. It currently detects the labels: Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title. As a reference (from the DocLayNet-paper), this is the performance of standard object detection methods on the DocLayNet dataset compared to human evaluation </p> </li> <li> <p>Table Former: The tableformer model will identify the structure of the table, starting from an image of a table. It uses the predicted table regions of the layout model to identify the tables. Tableformer has SOTA table structure identification.  The Arxiv paper can be found at the following link.  </p> </li> </ul> <p>So we have a lot to figure out here.  But I am convinced this is the way to go for an out-of-the-box methodology.  There is still a learning curve to optimize how to use the chunking module to optimize the vector embeddings.  </p>"},{"location":"blog/logging/","title":"Logging with Python's Standard Library","text":"<p>Date: March 6, 2025  Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>Logging, or tracking what happens as your code executes is pretty important to knowing what happened when things don't go as planned.  I have learned it is like inserting print statements in your code, but these statement can print out to the console (terminal or stdout), to a file, to an external logger (such as Logfire, which I will talk about in another post).  </p> <p>The bottom line is that you can write an awful lot of code in a notebook, or in small modules, and not have to track things.  But when things start getting big, with thousands of lines of code, you really need to be able to log what is going on especially to find bugs etc.  </p> <p>Plus, learning about logging makes you feel like a pro, like you have entered the big-time.  So lets get logging.  </p>"},{"location":"blog/logging/#pythons-logging-module","title":"Python's Logging Module","text":""},{"location":"blog/logging/#installing","title":"Installing","text":"<p>There is nothing to install, the logging module comes standard with Python!  You just need to add the following at the top of your files. <pre><code>import logging\n</code></pre></p>"},{"location":"blog/logging/#setup-logging-config","title":"Setup Logging Config","text":"<p>So I started by following a few tutorials.  The one that helped me the most was from mCoding and called Modern Python Logging.  Click on thumbnail below to be redirected to the video.  </p> <p> </p>"},{"location":"blog/markdown/","title":"\ud83d\udcc4 Markdown Cheatsheet","text":"<p>Date: April 27, 2025   Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>Markdown is a lightweight markup language.  It is a way to annotate plain text to indicate formating (like headings, bold, links etc), without writing full HTML tags.  </p> <p>It results in nicely rendered plain text, is simple, and totally yours to do what you want with.  We are using Mkdocs for our documentation, which creates a site from markdown files.  </p> <p>If you are not familiar with markdown, dont worry!  Below is a simple cheatsheet to help you along.  At the very bottom of this page there is also some links to other resources.  </p>"},{"location":"blog/markdown/#toc","title":"TOC","text":"<ul> <li>Headings</li> <li>Emphasis</li> <li>Lists</li> <li>Links</li> <li>Images</li> <li>Code</li> <li>Blockquote</li> <li>References and Links</li> <li>Horizontal Line</li> </ul>"},{"location":"blog/markdown/#headings","title":"Headings","text":"<pre><code># Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n</code></pre>"},{"location":"blog/markdown/#heading-1","title":"Heading 1","text":""},{"location":"blog/markdown/#heading-2","title":"Heading 2","text":""},{"location":"blog/markdown/#heading-3","title":"Heading 3","text":""},{"location":"blog/markdown/#heading-4","title":"Heading 4","text":""},{"location":"blog/markdown/#emphasis","title":"Emphasis","text":"<pre><code>*Italic*  \n**Bold**  \n***Bold and Italic***  \n~~Strikethrough~~\n</code></pre> <p>Italic Bold Bold and Italic ~~Strikethrough~~</p>"},{"location":"blog/markdown/#lists","title":"Lists","text":""},{"location":"blog/markdown/#unordered-list","title":"Unordered List","text":"<pre><code>- Item 1\n- Item 2\n    - Subitem 2a\n    - Subitem 2b\n</code></pre> <ul> <li>Item 1</li> <li>Item 2<ul> <li>Subitem 2a</li> <li>Subitem 2b</li> </ul> </li> </ul>"},{"location":"blog/markdown/#ordered-list","title":"Ordered List","text":"<pre><code>1. First\n2. Second\n3. Third\n</code></pre> <ol> <li>First  </li> <li>Second  </li> <li>Third</li> </ol>"},{"location":"blog/markdown/#links","title":"Links","text":"<pre><code>[Link Text](https://example.com)\n</code></pre> <p>Link Text</p>"},{"location":"blog/markdown/#images","title":"Images","text":"<pre><code>![Alt Text](https://via.placeholder.com/150)\n</code></pre>"},{"location":"blog/markdown/#code","title":"Code","text":""},{"location":"blog/markdown/#inline-code","title":"Inline Code","text":"<pre><code>Here is `inline code`.\n</code></pre> <p>Here is <code>inline code</code>.</p>"},{"location":"blog/markdown/#code-block","title":"Code Block","text":"<p>Use tripple backticks</p> <p>Rendered:</p> <pre><code>def hello():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"blog/markdown/#blockquote","title":"Blockquote","text":"<pre><code>&gt; This is a blockquote.\n&gt; It can span multiple lines.\n</code></pre> <p>This is a blockquote. It can span multiple lines.</p>"},{"location":"blog/markdown/#horizontal-line","title":"Horizontal Line","text":"<pre><code>---\n</code></pre>"},{"location":"blog/markdown/#references-and-links","title":"References and Links","text":"<ul> <li>\ud83d\udcc4 Markdown Guide - Basic Syntax</li> </ul> <p>Extremely clear and focused just on the main things: headings, bold, italic, lists, links, images, code, blockquotes.</p> <ul> <li>\ud83d\udcc4 Github Markdown-Cheatsheet</li> </ul> <p>A little bit longer but very practical \u2014 shows both the syntax and examples side-by-side.</p> <ul> <li>\ud83d\udcc4 Interactive markdown Tutorial</li> </ul> <p>If you like practice-as-you-learn, this one is great. It walks you through very short lessons where you type Markdown directly into the page.</p>"},{"location":"blog/markdown/#end-of-cheatsheet","title":"\u2705 End of Cheatsheet","text":""},{"location":"blog/mkdocs/","title":"Starting with MkDocs - Love at first use","text":"<p>Date: March 8, 2025   Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>I am sitting here in the St Agnus NYPL Library, where I spent a lot of the past week, writing code.  This has been an incredible journey.  It was only 6 days ago, that I took the time to try and understand how to use Cursor, the VSCode AI developer IDE.  [With Cursor] I have written about 3000 lines of working code, developing the pipeline for Nanobot.  </p> <p>I now need to start with documentation, and I chose to start with MkDocs, and it is rather nice!  </p>"},{"location":"blog/mkdocs/#project-documentation-with-markdown","title":"Project documentation with Markdown","text":""},{"location":"blog/mkdocs/#what-it-does","title":"What it Does","text":"<p>You can find it at MkDocs Pages, and the Github repo is at Github MkDocs</p> <p>The key is that you can do all of the documentation in markdown, which makes understanding and updating easy.  It also gives you a very nice web interface, serving a site and can also deploy to github.io to make the docs part of your repo.  Mindblowing \ud83e\udd2f.</p>"},{"location":"blog/mkdocs/#choosing-your-theme","title":"Choosing Your Theme","text":"<p>The first thing to do is to choose your theme.  There are two built-in themes: * mkdocs * readthedocs</p> <p>There are also many third part themes, which you are advised to use at your own risk.  However, there is another which is highly used and well suported and documented, which has 22k stars and is written by squidfunk:</p> <ul> <li>material</li> </ul> <p>This is what I have decided to use</p>"},{"location":"blog/mkdocs/#choosing-your-plugins","title":"Choosing Your Plugins","text":"<p>Plugins are sometimes third party items.  Be careful.  I tried the mkdocs-video and it did weird stuff and I uninstalled it.  In any event there is a list of Themes and Plugins at the MkDocs Catalog in the README.  </p>"},{"location":"blog/mkdocs/#how-to-install-it","title":"How to install it","text":"<p>There were only 3 pip installs that I had to do.  the <code>mkdocstrings[python]</code> will evidently read your docstrings and create the API docs from them, but I have not used this feature yet so will update when I get there.  </p> <pre><code># Core MkDocs package\npip install mkdocs\n\n# Material theme for MkDocs\npip install mkdocs-material\n\n# MkDocstrings for API documentation from docstrings\npip install mkdocstrings[python]\n</code></pre>"},{"location":"blog/mkdocs/#yml-file-structure","title":"YML File Structure","text":"<p>You will also need a YML file with the configuration <code>mkdocs.yml</code>. My current YAML file looks like:</p> <pre><code>site_name: NanoBot Documentation \ud83e\udd16\nsite_description: Documentation for the NanoBot POC project\nsite_author: Your Name\n\n# Standard docs directory\ndocs_dir: docs\n\ntheme:\n  name: material\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n\n    # Dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.instant\n    - toc.integrate\n    - search.suggest\n    - search.highlight\n\nmarkdown_extensions:\n  - pymdownx.highlight\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.tasklist\n  - admonition\n  - toc:\n      permalink: true\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n  - md_in_html\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          selection:\n            docstring_style: google\n          rendering:\n            show_source: true\n            show_root_heading: true\n\nnav:\n  - Home: index.md\n  - Getting Started: getting-started.md\n  - User Guide:\n    - Overview: user-guide/index.md\n    - Document Processing: user-guide/document-processing.md\n    - Vector Search: user-guide/vector-search.md\n  - API Reference:\n    - Database: api/database.md\n    - Services: api/services.md\n  - Examples: examples.md\n  - Blog: \n    - Overview: blog/index.md\n    - MkDocs: blog/mkdocs-2025-03-08.md\n</code></pre>"},{"location":"blog/mkdocs/#docs-file-structure-holds-all-of-your-markdown-files","title":"docs File structure (Holds all of your Markdown files)","text":"<p>This is my current dir structure.  I place the <code>/docs</code> directory in the root of my nanobot-poc project.  </p> <p>NOTE: The <code>index.md</code> files are necessary</p> <pre><code>docs/\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 database.md\n\u2502   \u2514\u2500\u2500 services.md\n\u251c\u2500\u2500 blog/\n\u2502   \u251c\u2500\u2500 post-2025-03-08.md\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 img/\n\u2502   \u251c\u2500\u2500 nanobot_schema.jpg\n\u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 document-processing.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 vector-search.md\n\u251c\u2500\u2500 examples.md\n\u251c\u2500\u2500 getting-started.md\n\u2514\u2500\u2500 index.md\n</code></pre>"},{"location":"blog/mkdocs/#how-to-use-it","title":"How to use it","text":"<p>I havent worked with this but more than 1 hour now, but here is what I know:</p> <p>You can serve the pages locally by running the following command from the root <pre><code>mkdocs serve\n</code></pre> This will launch a local server on port 8000 <code>http://127.0.0.1:8000/</code> where you interact with the HTML version of it.  It is evidently a Bootstrap implementation.  </p> <p>NOTE: If you change the YML file you will need to stop the server <code>ctl C</code> and then restart it again by <code>mkdocs serve</code></p> <p>You can add documents to this but to get them seem in the navigation sidebar you need to add them to the YML file.  If you know the file name you can still navigate to it using the URL but you will not see it in the <code>nav</code> sidebar.  </p> <p>You can deploy this to github.  In the case of my own repo of <code>nanobot-poc</code> I would navigate to <code>srobertsphd.github.io/nanobot-poc</code> and this is where it would reside.  There is an order of events though. </p>"},{"location":"blog/mkdocs/#updating-github-repo","title":"Updating GitHub repo","text":"<ol> <li>Update your Markdown Docs</li> <li>Update you <code>mkdocs.yml</code> file to unclude any new docs</li> <li>Navigate to your project directory</li> <li>Push the updated mkdown to the repo <pre><code>git add docs/ mkdocs.yml\ngit commit -m \"updated documentation\"\ngit push\n</code></pre></li> <li>Deploy to GitHub Pages using MkDocs  </li> </ol> <pre><code>  mkdocs gh-deploy\n</code></pre>"},{"location":"blog/mkdocs/#important-notes","title":"Important Notes","text":"<ul> <li>If your repository is private, you'll need a GitHub Pro, Team, or Enterprise account to use GitHub Pages  </li> <li>It may take a few minutes for your site to be available after deployment  </li> <li>You can check the status of your GitHub Pages deployment in your repository settings</li> </ul>"},{"location":"blog/mkdocs/#content-tabs","title":"Content tabs","text":"<p>The indenting here really matters -- needs to be double tabbed</p> <p>this is the markdown for the three tabs below</p> <pre><code>=== \"plain Text\"\n\n    This is some plain text with code\n\n    ```python\n    print(\"hello world\")\n    ```\n\n=== \"Unordered List\"\n\n    * first\n    * second\n    * third\n\n=== \"Ordered List\"\n\n    1. first\n    2. second\n    3. third\n</code></pre> <p>And this renders as the three tabs below</p> plain TextUnordered ListOrdered List <p>This is some plain text with code</p> <pre><code>print(\"hello world\")\n</code></pre> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <ol> <li>first</li> <li>second</li> <li>third</li> </ol>"},{"location":"blog/mkdocs/#admonishions","title":"Admonishions","text":"<p>Examople of an admonition/callout with a title:</p> <p>Note the types here -- there is also * note * info * success * tip</p> <p>Each have their own icons.  check it out!</p> <p>Title of the Callout</p> <p>this is some text that is indented with 2 tabs.  here is some more text.  also i wanted to say hello world.  this is an amazing place to be these days</p> Title of the COLLAPSIBLE Callout [CLICK on me] <p>this is some text that is indented with 2 tabs.  here is some more text.  also i wanted to say hello world.  this is an amazing place to be these days</p>"},{"location":"blog/mkdocs/#diagram-examples","title":"Diagram Examples","text":""},{"location":"blog/mkdocs/#flowcharts","title":"Flowcharts","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Failure?};\n  B --&gt;|Yes| C[Investigate...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Success!];</code></pre>"},{"location":"blog/mkdocs/#adding-mathjax-for-equation-formatting-and-display","title":"Adding MathJax for Equation formatting and Display","text":"<p>Taken from the documents -- you will need to add a javascripts folder with the following code, and also modify the mkdocs.yml file as below:</p> docs/javascript/mathjax.jsmkdocs.yml <pre><code>window.MathJax = {\n  tex: {\n    inlineMath: [[\"\\\\(\", \"\\\\)\"]],\n    displayMath: [[\"\\\\[\", \"\\\\]\"]],\n    processEscapes: true,\n    processEnvironments: true\n  },\n  options: {\n    ignoreHtmlClass: \".*|\",\n    processHtmlClass: \"arithmatex\"\n  }\n};\n\ndocument$.subscribe(() =&gt; { \n  MathJax.startup.output.clearCache()\n  MathJax.typesetClear()\n  MathJax.texReset()\n  MathJax.typesetPromise()\n})\n</code></pre> <pre><code>markdown_extensions:\n  - pymdownx.arithmatex:\n      generic: true\n\nextra_javascript:\n  - javascripts/mathjax.js\n  - https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js\n</code></pre>"},{"location":"blog/mkdocs/#closing-comments","title":"Closing Comments","text":"<p>Writing code is so much fun. Learning new libraries in minutes is amazing fun<sup>1</sup>. Leaning how to document things and start writing in less than an hour and having it look good and be accessible --&gt; INCREDIBLE good feeling<sup>2</sup>.</p> <p>This is a pretty sweet gig.  </p>"},{"location":"blog/mkdocs/#references","title":"References","text":"<ol> <li> <p>James Willet Dev Full Tutorial To Build And Deploy Your Docs Portal  https://www.youtube.com/watch?v=xlABhbnNrfI\u00a0\u21a9</p> </li> <li> <p>James Willet Dev Getting Started with Material for MkDocs -- https://jameswillett.dev/getting-started-with-material-for-mkdocs/ \u21a9</p> </li> </ol>"},{"location":"blog/packages-and-imports/","title":"Managing Packages and Imports","text":"<p>Date: April 25, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre>"},{"location":"blog/packages-and-imports/#cleaning-up-your-virrual-environment","title":"Cleaning up your virrual environment","text":"<p>One always wants to would in a development environment.  most often this is done by creating an environment that your codebase depends on using something like <code>venv</code>, which is what we are currently using in our projects.  </p> <p>One of the issues is that every time you do a <code>pip install [package name]</code> it will also install all of the dependencies that that package depends on.  However, in the requirements.txt file it is best practice to only list the top level dependencies, and let pip resolve and install all of the subdependencies.  </p> <p>If you are like me and have a requirements files that includes all of the dependencies including the sub-dependencies, and need to clean all of that up, you can do the following:</p> <ol> <li>Create a backup of your current requirements.txt file <pre><code>cp requirements.txt requirements.txt.bak\n</code></pre></li> <li>Install pipreqs in your activated virtual environment <pre><code>pip install pipreqs\n</code></pre></li> <li>Run pipreqs from the project root <pre><code>pipreqs . --force\n</code></pre></li> </ol>"},{"location":"blog/packages-and-imports/#importing-and-using-local-packages","title":"Importing and using local packages","text":"<p>This is a good reference for importing packages from a good python content creator with the following channel: Tech with Tim:</p> <p></p>"},{"location":"blog/packages-and-imports/#how-to-import-a-module-from-within-the-package","title":"How to import a module from within the package","text":"<p>As an example:  </p> <p>From the chunking.py file in the document_conversion folder, I want to import the Chunks and ChunkMetadata classes from the models.db_schemas module.</p> <pre><code>import sys\nimport os\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(parent_dir)\nfrom models.db_schemas import Chunks, ChunkMetadata\n</code></pre> <ol> <li>os.path.abspath(file): This gets the absolute path of the current file (chunking.py).</li> <li>os.path.dirname(): Called twice to move up two directory levels, reaching the root of your project (nanobot-poc).</li> <li>sys.path.append(parent_dir): Adds the parent directory to sys.path, which is the list of directories Python searches for modules. By including the parent directory, Python can now find models.db_schemas because models is a sibling directory to document_conversion.</li> <li>Import Statement: With the parent directory in sys.path, the import from models.db_schemas import Chunks, ChunkMetadata works because Python can now locate the models directory.</li> </ol> <p>This method effectively adjusts the module search path at runtime, allowing you to import modules from sibling directories.</p>"},{"location":"blog/parsing-docs/","title":"Document Conversion Tools for LLM Processing: A Comparative Evaluation","text":"<p>Date: May 11, 2025 Author: ChatGPT Deep Research</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>I have asked ChatGPT Deep Research to compare the most capable tools for converting complex scientific documents\u2014like PDFs, PPTs, and Google Docs\u2014into markdown, with support for OCR, tables, images, and heading structures. We want to compare both open-source and commercial tools (like IBM DocLing, Unstructured, LlamaIndex, and vision-based LLMs), assess their document parsing quality, formatting preservation, chunking strategies (semantic and structural), and Python ecosystem integration.</p> <p>Here is the summary:</p>"},{"location":"blog/parsing-docs/#introduction","title":"Introduction","text":"<p>Converting complex documents into clean, structured Markdown is crucial for use with Large Language Models. Scientific manuals, technical methods, and tool documentation often contain rich formatting (headings, tables, images, graphs, and even equations) that should be preserved for LLMs to understand context. This comparison evaluates leading document conversion tools \u2013 both open-source and commercial \u2013 focusing on their ability to produce Markdown output that is semantically or structurally chunked and suitable for LLM pipelines. We cover open-source tools like IBM Docling, Unstructured.io, LlamaIndex, and custom LLM vision models, as well as cloud platforms (e.g. AWS Textract, Azure Form Recognizer) where applicable. Key criteria include:</p> <ul> <li>Markdown Fidelity (preservation of headings, lists, tables, images, and math/LaTeX formatting)</li> <li>OCR Support for scanned PDFs and images</li> <li>Chunking Capabilities (semantic vs. structural chunking and customization)</li> <li>Integration with Python-based LLM workflows (LangChain, LlamaIndex, etc.)</li> <li>Support for Scientific/Technical Content (e.g. equations, diagrams, special symbols)</li> <li>Ease of Use &amp; Flexibility (installation, API/CLI availability, ecosystem support)</li> </ul> <p>Below, we examine each tool/category against these criteria, followed by a summary comparison table.</p>"},{"location":"blog/parsing-docs/#open-source-solutions","title":"Open-Source Solutions","text":""},{"location":"blog/parsing-docs/#ibm-docling","title":"IBM Docling","text":"<p>IBM\u2019s Docling is a modern open-source toolkit (MIT-licensed) explicitly designed to convert a wide range of document formats (PDF, DOCX, PPTX, HTML, etc.) into structured Markdown or JSON. It was released in mid-2024 and quickly gained popularity for its output quality \u2013 \u201cThe output quality is the best of all the open-source solutions,\u201d noted one developer. Docling emphasizes layout preservation: it uses computer vision models to detect page structure (text blocks, headings, columns, images, tables, captions, footnotes) rather than relying solely on OCR. This approach yields Markdown that retains the original formatting (including proper headings, lists, and multi-column order) for high fidelity. Tables are handled via an advanced model (TableFormer) that reconstructs tables into Markdown (or structured data) with accurate rows and columns. Images and figures are extracted and referenced so that the Markdown can embed them (often via <code>![]()</code> with either local paths or base64) \u2013 ensuring diagrams or screenshots in manuals are not lost.</p> <p>OCR and Scanned PDFs: Docling avoids OCR when text is digitally accessible, which \u201creduces errors and speeds up processing by \\~30\u00d7\u201d. For scanned pages or images, it can integrate with OCR engines. Out of the box, it supports EasyOCR and Tesseract for text recognition. However, default OCR (EasyOCR) is relatively slow on CPU. Docling\u2019s design allows swapping in better OCR models; for example, developers have used RapidOCR (optimized OCR models) with Docling to improve speed and accuracy. Still, Docling\u2019s strength is in \u201cborn-digital\u201d documents; it struggles more with purely scanned or handwritten documents. The creators acknowledge plans to expand Docling\u2019s capabilities for math equations, charts, and forms in the future, but currently those may not be fully parsed (equations might come through as images or unstructured text).</p> <p>Chunking: Docling outputs a structured Markdown (often as a single document) with preserved hierarchy (it keeps the heading structure, etc.). For large documents, it can optionally split output by page or section. In fact, Docling\u2019s Python API returns a rich <code>DoclingDocument</code> object, which tools like LlamaIndex can further split into chunks by pages or headings. Because Docling maintains the document\u2019s logical structure, it pairs well with structural chunking \u2013 e.g. splitting by Markdown headings or sections is straightforward. Users have noted Docling \u201callows splitting [by page] out of the box\u201d, making it compatible with external chunkers. In RAG pipelines, one might use Docling to convert to Markdown and then apply a chunking function (by heading or token size) to create LLM-ready chunks.</p> <p>Integration: Docling is a Python library and also offers a CLI. Installation and use are straightforward (the IBM blog boasts it takes \u201cjust five lines of code\u201d to set up). It\u2019s designed to integrate into LLM workflows: it works with LangChain and LlamaIndex natively. For example, LlamaIndex provides a <code>DoclingReader</code> that uses Docling under the hood so that documents can be directly ingested as Markdown with layout information. This makes it easy to plug Docling into an existing Python-based pipeline for question-answering or fine-tuning. Being open-source and local (no external API calls required), it\u2019s flexible for enterprise use and can run on-prem. Note that Docling\u2019s models (for layout and tables) do make it a heavier dependency \u2013 it requires downloading model weights and can use GPU for acceleration. Users report it is \u201csuper heavy, but close to perfect\u201d in output. Still, it runs on a standard laptop and has been optimized for decent performance given its complex tasks.</p> <p>Scientific/Technical Content: Docling was partly developed on technical documents (patents, manuals, etc.), so it performs well on structured scientific text, complex layouts, and technical tables. It classifies and preserves captions, footnotes, equations (as images), and other elements better than simple extractors. Math formulas in PDFs that are text (e.g. LaTeX in PDF) might be captured as text, but formatting could be lost; truly rendering LaTeX in Markdown is not yet solved, though support is a \u201cwhat\u2019s next\u201d item. For now, one might need an external tool (like Mathpix) to convert equation images to LaTeX if needed. Overall, Docling\u2019s Markdown output is very clean and keeps semantic structure (sections, lists, etc.) intact, which is highly beneficial for LLM context.</p> <p>Ease of Use: With both CLI and Python API, and good documentation, Docling is relatively easy to use. It\u2019s under active development (community contributions welcome) and has already been integrated into popular frameworks. The trade-off for its high fidelity is its computational load \u2013 running multiple specialized models (layout analysis, table parsing, OCR) means it\u2019s not the fastest lightweight tool. However, IBM has demonstrated its scalability: Docling processed millions of PDF pages for projects like CommonCrawl data extraction and is being used in IBM\u2019s products. For a developer, installing the <code>docling</code> package and calling <code>DocumentConverter.convert()</code> yields a Markdown file with minimal effort. In summary, Docling is a top choice for maximum Markdown fidelity on complex docs, especially if they are digitally generated, and it integrates well into Python LLM workflows. Its current limitations are mainly with low-quality scans and the nascent support for things like equations or handwriting.</p>"},{"location":"blog/parsing-docs/#unstructuredio","title":"Unstructured.io","text":"<p>Unstructured is an open-source library (with an optional cloud service) that provides components to ingest and process documents into structured data, primarily aimed at RAG use cases. It became popular early (integrated with LangChain in 2022) as a way to parse PDFs and other files. Unstructured\u2019s approach is to \u201cpartition\u201d documents into semantic elements \u2013 paragraphs, headings, lists, table chunks, images, etc. \u2013 using format-specific logic. These elements can then be recombined or \u201cchunked\u201d for LLM consumption. Unstructured supports a wide range of file types (PDF, DOC(X), PPT(X), images, HTML, etc.) and can output content as plain text, JSON (with element metadata), or HTML/Markdown as needed. While it doesn\u2019t natively produce a single monolithic Markdown file with all formatting preserved in the same way Docling does, it captures structure in its element outputs which can be serialized to Markdown or HTML. For example, it can extract tables and represent them in HTML or markdown table format, and pull images as base64 strings.</p> <p>In an evaluation of PDF parsers, Unstructured\u2019s output was considered very high quality \u2013 preserving context and order. It smartly handled artifacts like footers/headers by not conflating them with body text, which is \u201clikely to benefit RAG systems\u201d (preventing irrelevant text from breaking chunks). It correctly keeps multi-column text separated (so text reading order is correct) in most cases, and identifies headings vs paragraphs accurately. One review gave Unstructured a score of \u201cExcellent\u201d for faithfully extracting text in a way that is chunk-ready. The library also attempts to preserve formatting: headings in PDF -&gt; heading elements (which one can render as <code>#</code> in Markdown), lists remain lists, and tables can be extracted as structured data or HTML. However, integrating images into markdown requires an extra step: Unstructured can extract images (as files or base64) but you must place them into the Markdown manually (e.g., with <code>![Alt text](data:image/png;base64,...)</code>). The same goes for tables: it can output HTML for tables, which one might convert to Markdown table syntax.</p> <p>OCR and Scanned Document Support: Unstructured will use OCR when needed. In fact, \u201cfor text, Unstructured works quite well, quickly processing easy documents while falling back to OCR when required\u201d. It uses the layoutparser and Tesseract by default for image-based PDFs. The open-source version handles basic OCR but may not be as accurate or fast on heavy OCR tasks. To address this, Unstructured.io provides cloud tiers: Advanced and Platinum modes use better OCR and even handwriting recognition (the Platinum tier, $30/1k pages, can handle handwriting). These are commercial endpoints but show that the ecosystem supports more difficult inputs. For open-source users, one can also configure a custom OCR in Unstructured\u2019s pipeline (similar to how Docling can integrate alternatives). In summary, Unstructured can process scanned PDFs, but complex cases (handwritten forms, low-quality scans) may need the higher-tier service. On purely digital PDFs, it often avoids OCR and pulls text directly (e.g., via PDFMiner or PyMuPDF), preserving accuracy.</p> <p>Chunking Capabilities: Chunking is a core concept in Unstructured\u2019s design. It operates at the element level \u2013 after partitioning a document into its elements, you can specify a chunk size (in characters or tokens) and the library will combine elements without breaking them until the size is met. This ensures chunks are semantically coherent (e.g., a heading stays with its subsequent paragraph, multiple small paragraphs can be in one chunk). Only if an individual element is larger than the max size will it split that element by text length. This approach yields semantic/structural chunking by default, as opposed to naive splitting on every N tokens or newline. There are options to tweak chunking strategies \u2013 for example, you might want each top-level section as one chunk, or limit tables to their own chunk, etc. Unstructured\u2019s documentation highlights that it preserves semantic units during chunking, unlike simpler newline-based splitters. This makes it very suitable for LLM pipelines, as each chunk is a logical piece of the document (improving LLM understanding). You can also disable chunking if you just want the full Markdown.</p> <p>Integration: Unstructured is available as a Python package (<code>unstructured</code>), and it also offers a REST API (self-hosted or hosted). It was one of the first to integrate with LangChain (as a document loader) and can be used with LlamaIndex or any Python code. The Partition API returns Python dataclasses for each element (with metadata like type, text, coordinates). Converting those to Markdown is a matter of iterating through elements and mapping types to Markdown syntax (some community tools do this already). Additionally, Unstructured\u2019s \u201cstaging\u201d module can convert element lists to JSON or other schema, which can be helpful if you need a custom format. The ecosystem around Unstructured is growing \u2013 it\u2019s used in many RAG pipelines, and there are third-party projects that extend it (for example, Pathway\u2019s doc parser uses Unstructured and Docling under the hood). The open-source is free; the hosted version is pay-per-page with high throughput. Overall, integration is straightforward: a few lines to call <code>partition_pdf(filename)</code> and then handle the resulting elements.</p> <p>Scientific/Technical Document Support: Unstructured can handle technical documents reasonably well. It will extract text and tables from PDFs of research papers or manuals, and it can also handle images (it can extract images and you could use an image captioning model afterwards if needed). However, it does not have special handling for math equations \u2013 if the PDF encodes an equation as text, it will appear, but possibly not in LaTeX format (just as characters or an image). If the equation is an image, Unstructured would treat it as an image element (with no built-in OCR specifically for math). So for scenarios heavy in math notation, additional steps are needed (Mathpix API or similar can complement the pipeline). That said, Unstructured\u2019s general parsing of scientific docs (with multiple sections, figures, citations, etc.) is strong in terms of keeping sections separate and maintaining order. It was designed for semi-structured data like financial reports, scientific papers, etc. (in fact, it\u2019s often used for legal/financial PDF processing). It might not preserve all formatting (like bold or italic text would just come through as plain text), whereas Docling might wrap those in Markdown syntax. So Markdown fidelity for rich text style is a bit lower: Unstructured\u2019s focus is structural fidelity (what is a heading vs body), rather than stylistic. For most LLM use cases, this is acceptable.</p> <p>Ease of Use: Installing <code>unstructured</code> is straightforward, but it may pull in many dependencies (it\u2019s a modular library that can use various OCR or PDF backends). The documentation is extensive, and there are examples for common tasks. Basic usage is simple, but to get images or other advanced features, you need to dive into options (like <code>extract_image_contents=True</code>, etc.). The library is actively maintained by Unstructured.io and has an open community. One consideration is that for large volumes of documents, using the self-hosted or cloud Workflow Endpoint may be easier (the Workflow API can orchestrate chunking, embedding, etc., end-to-end). Flexibility is high: you can use it locally or as a service, and it supports many formats out of the box.</p> <p>In summary, Unstructured.io provides a highly flexible and effective way to turn complex documents into Markdown-friendly chunks. It excels at structural and semantic chunking and handles many document types. Its Markdown fidelity is good with structure (headings/tables), though it may not explicitly italicize or render formulas. For scanned docs, it does the job but might require higher-tier services for best results. Many users pair Unstructured with LLM pipelines successfully, thanks to its coherent output.</p>"},{"location":"blog/parsing-docs/#llamaindex-and-llamaparse","title":"LlamaIndex (and LlamaParse)","text":"<p>LlamaIndex (formerly GPT Index) is not a document conversion tool per se, but rather a framework for LLM data indexing and retrieval. It provides connectors to ingest documents and build indices for querying with LLMs. In doing so, LlamaIndex has to parse documents, chunk them, and possibly preserve structure. The open-source LlamaIndex includes various <code>DocumentReader</code> classes: for example, a PDF reader that uses PyMuPDF to extract text, or integrations with Unstructured and Docling. By default, earlier versions of LlamaIndex would extract text from PDFs (losing images and some formatting). Users noted that \u201cLlamaIndex (default) has no integrated images\u201d when converting PDFs, and sometimes text order issues (likely due to underlying PDF extraction). To address complex docs, LlamaIndex has evolved to support external parsers:</p> <ul> <li>DoclingReader: As mentioned, LlamaIndex can leverage Docling to produce rich Markdown and then split that into Nodes for indexing. This gives LlamaIndex-powered apps the same fidelity as Docling\u2019s output.</li> <li>UnstructuredReader: Similar integration exists to use Unstructured.io\u2019s output in LlamaIndex.</li> <li>SimpleDirectoryReader: A basic reader that can handle text and some PDFs, but without fancy formatting.</li> </ul> <p>Additionally, LlamaIndex introduced LlamaParse as part of their cloud offering (LlamaHub/LlamaCloud). LlamaParse is a hosted document parsing service. It has a Basic tier and a Premium tier; the premium one is aimed at maximum accuracy (and is quite expensive, \\~$45/1k pages). LlamaParse presumably uses advanced ML (possibly their own models or a combination of methods) to extract PDFs into Markdown. In a direct comparison, LlamaParse was found to accurately extract tables and content but had some issues with layout in certain cases. For instance, one test showed LlamaParse mixing columns of text together, treating multi-column text as one continuous block (causing \u201cgibberish\u201d output in that segment). It also missed some headings by merging a heading line into the following paragraph. These issues led a reviewer to rate LlamaParse\u2019s layout handling as \u201cFair\u201d, whereas Unstructured was \u201cExcellent\u201d. However, LlamaParse did very well on tables, earning an \u201cExcellent\u201d score on table extraction quality. It generally recognizes tables, even complex ones, similarly to Docling\u2019s performance. So, LlamaParse is strong but not infallible; it may be improving rapidly given it's a managed service.</p> <p>Markdown Fidelity: With the new integrations, LlamaIndex can achieve high markdown fidelity by using Docling or Unstructured backends. On its own, the core library focuses more on text content. LlamaParse output is in Markdown format (with images as links to cloud storage, etc.) \u2013 all three top parsers compared (Unstructured, LlamaParse, Vectorize) \u201care able to generate a markdown representation of the content,\u201d preserving headings, images, graphs, formatting. This is essential for LLM usage, and LlamaIndex\u2019s philosophy aligns with that (keeping context). If using just LlamaIndex\u2019s basic loader, you might get only plain text. But with minimal configuration, one can use the advanced readers to get structured Markdown. LlamaIndex itself will break documents into nodes (chunks) for you, typically aiming at \\~512 tokens per chunk by default. You can configure chunk size and overlap easily. By default this is a structural/semantic chunking hybrid \u2013 it tries to chunk at paragraph or section boundaries when possible (it won\u2019t cut off mid-sentence unless necessary). The integration with Docling means it can even chunk by the Markdown headings that Docling provides, which is very handy (each section becomes a node, etc.).</p> <p>OCR support: LlamaIndex itself defers OCR to whatever reader is used. If you use the built-in PDFReader (PyMuPDF), that will only get text from PDFs with a text layer (no OCR). If you need OCR, you\u2019d incorporate an OCR-based reader (Unstructured, Docling, or even a custom function using pytesseract). LlamaParse cloud likely has OCR in the backend for scanned PDFs, though details aren\u2019t public. In practice, if you need to handle scanned documents in a LlamaIndex pipeline, you would call an OCR service first or use a tool like Docling which will handle it (Docling + RapidOCR for example).</p> <p>Chunking and Integration: Chunking is one of LlamaIndex\u2019s core strengths. Once the document is in Markdown or text form, LlamaIndex can apply structure-based splitting (e.g., split by headings, by semantic units if using their <code>NodeParser</code>). The new NodeParser concept in LlamaIndex allows custom parsing of a Markdown document into semantic nodes. For example, they provide a <code>MarkdownNodeParser</code> that can split a markdown by headings, subheadings, etc., automatically tagging each node with the section hierarchy (very useful for retrieval context). This means if you feed LlamaIndex a Markdown manual with <code>#, ##, ###</code> headings, you can get each subsection as a chunk with metadata indicating its position. LlamaIndex also supports keyword-based or semantic splitting if you want, but most often hierarchical splitting is used. Integration-wise, LlamaIndex is already the integration layer \u2013 it\u2019s meant to plug into any LLM backend (OpenAI, local models) and vector stores. So using LlamaIndex means you are already in Python, and you can treat any of the above tools as a preprocessor stage or use the built-in connectors to do it seamlessly.</p> <p>Scientific/Technical support: With the appropriate parser, LlamaIndex can ingest scientific documents well. It doesn\u2019t do any special transformation on its own, so the capability depends on the underlying conversion. For example, to handle a chemistry PDF with formulas and images, one might use Docling to get the content (Docling will label images and preserve table structure, but might leave chemical equations as they are). LlamaIndex would then chunk and index that content. One advantage of LlamaIndex is you can enrich the document nodes with embeddings or metadata (like \u201cthis chunk came from Chapter 2\u201d etc.) to aid retrieval. But in terms of conversion fidelity, LlamaIndex by itself doesn\u2019t improve it; it leverages others. In summary, think of LlamaIndex as the orchestrator: it ensures whatever Markdown you get is fed correctly into an index. It now has good ecosystem support to use the best converters available (Docling integration is a prime example).</p> <p>Ease of Use: For those already using LLM pipelines, LlamaIndex is quite user-friendly. Installing it (<code>pip install llama-index</code>) and using a reader like DoclingReader requires also installing the corresponding package (<code>llama-index-readers-docling</code> etc.). The documentation and community examples are solid. However, using LlamaIndex just for conversion might be overkill if you don\u2019t need indexing or querying. Its value is more in the end-to-end pipeline (convert -&gt; embed -&gt; query). If we consider LlamaParse (cloud) as part of this category, that requires an API key and perhaps using their web interface or SDK. LlamaParse could be an option for those who want a managed solution without hosting their own Docling/Unstructured. But one must consider cost and data privacy (sending docs to a cloud service). LlamaIndex open-source gives you flexibility to choose open or closed solutions under one umbrella. Overall, LlamaIndex ensures tight integration with Python-based LLM workflows (since it is such a workflow tool), and via its modules you can achieve conversion, chunking, and integration in one go.</p>"},{"location":"blog/parsing-docs/#custom-llm-vision-models","title":"Custom LLM Vision Models","text":"<p>Another approach to document conversion is to leverage multimodal Large Language Models or vision-enabled LLMs. This is a less traditional \u201ctool\u201d \u2013 it involves prompting an LLM (like GPT-4 Vision, or open-source multimodal models) with the document (as images or text) and asking it to produce Markdown. Essentially, the LLM itself acts as the parser, potentially handling complex content in a flexible way. Recent advances (e.g., GPT-4\u2019s vision feature, or Google\u2019s upcoming Gemini, and open models like LLaVA, BLIP-2, etc.) have made it possible to feed in a document page image and get a natural language (or structured) description of it.</p> <p>Markdown Fidelity: A capable vision+language model can, in theory, output not just plain text but well-formatted Markdown if instructed. For example, one could prompt GPT-4 with: \u201cYou are an assistant that converts document images to Markdown. Preserve all headings, bold text, tables (use Markdown table syntax), and describe images.\u201d The model might then generate Markdown that approximates the layout. This approach could capture semantic meaning and even describe non-textual elements (like \u201cGraph: a plot of X vs Y showing ...\u201d). It could also transcribe math equations by interpreting them, possibly yielding LaTeX if specifically asked. However, there are challenges: the LLM might hallucinate or omit content if the image is unclear or if it misunderstands something. Unlike deterministic parsers, an LLM might make minor errors (e.g., misread a number and \u201cguess\u201d it, or rephrase text). There is a trade-off: LLMs can handle ambiguity and context (reading handwriting better through context, for instance), but ensuring 100% faithful conversion is difficult. IBM researchers noted that their approach with Docling avoids the risk of hallucination by not relying on a generative model for conversion. So, while a custom LLM approach is flexible, one must carefully verify the output.</p> <p>OCR and Scanned Support: Using an LLM for vision is essentially an advanced form of OCR+understanding. For purely scanned documents, a strong vision model can definitely parse the text (like OCR) and also interpret layout. Some users have reported success replacing traditional OCR pipelines with GPT-4 Vision, because it was \u201cso darn easy\u201d and effective in extracting information. Similarly, a comment on HN mentioned using Google Gemini 2.0 to replace an OCR vendor, noting that with an LLM \u201cyou have full control over the schema\u2026 the problem shifts from can we extract this to can we fit it in the context window\u201d. Essentially, an LLM can be instructed to output whatever structure you want (JSON, Markdown, etc.), which is very powerful. This means you can ask it to output exactly the Markdown format needed, including special handling (like \u201cuse LaTeX for any equations\u201d). For scanned multi-page PDFs, one might iterate page by page (since image input per page) or use a high context multimodal model that accepts a whole document (if such becomes available). Current GPT-4 Vision is limited to images one at a time and may have input size limits.</p> <p>Chunking: With an LLM approach, chunking is somewhat intrinsic \u2013 you\u2019d likely process one page or section at a time (unless the model supports very long image sequences). You can also let the model decide logical chunks: e.g., \u201cread the document and split it into sections with Markdown headings accordingly.\u201d The LLM could output a naturally chunked Markdown where each section is clearly delineated. If needed, you could then further split by heading level. Since you \u201ccontrol the schema,\u201d you could instruct the model on how to segment content. This is more of a manual approach compared to the built-in chunking functions of other tools.</p> <p>Integration: Using LLMs for this requires access to the model. For GPT-4 Vision, that means an OpenAI API call with image, which is not yet widely general-purpose (and has costs and rate limits). There are open-source vision models (like BLIP-2, Donut, or LLaVA) that you can run locally, but their accuracy may be lower. For instance, Donut is a model specifically for document OCR without explicit OCR (an end-to-end transformer that outputs text from an image). Donut can be fine-tuned on documents to output Markdown or JSON. Such custom models could be integrated in Python pipelines (HuggingFace has implementations). However, doing this at scale might be slow or require GPU resources. The integration complexity is higher than using a ready library \u2013 you need to handle model inference and possibly prompt engineering. That said, frameworks like LangChain can incorporate an LLM with vision as a tool in an agent chain (e.g., feed each page image to the model and get text).</p> <p>Scientific/Technical support: LLMs might actually shine here. A powerful model could read an equation and output it in $\\LaTeX$ (if trained or if it can visually parse math notation \u2013 GPT-4 has been shown to output LaTeX for equations in some cases). It could also interpret diagrams and graphs in a meaningful way (though that veers into summarization rather than exact conversion). If the goal is strictly conversion, one might not want interpretation, just transcription. But even transcription of math by an LLM is impressive compared to generic OCR which usually fails on complex notation. For chemical structures or figures, an LLM could at least label them or describe them in alt-text, which is value-add. No other tool will spontaneously add descriptive text for an image \u2013 they just give you the image itself. So in contexts where you want to enrich the document for an LLM (say, explain a figure), a vision LLM can do that. However, caution: for factual fidelity, it might mis-describe a complex diagram. As of now (2025), these models are cutting-edge but not 100% reliable for detailed conversion tasks.</p> <p>Ease of Use &amp; Flexibility: This approach is currently the most complex to implement and potentially the most expensive (if using paid API for GPT-4). It\u2019s \u201ceasy\u201d in the sense that you can give a single prompt \u201cConvert this to markdown\u201d and the model does a lot \u2013 but you have to manually oversee and possibly correct the output. There\u2019s no turn-key library that guarantees consistent results every time. Also, sending large documents to an API can be slow. On the plus side, it\u2019s very flexible: you can tailor the prompt to your specific document type (one can instruct a custom model to pay attention to certain details, or ignore certain sections).</p> <p>A likely best practice is to use LLMs in combination with other tools: e.g., use Docling or Unstructured to get a baseline Markdown (ensuring all text is captured exactly), then possibly have an LLM post-process that Markdown to, say, insert backticks around code snippets, or convert certain notations to LaTeX. This way, you get the accuracy of deterministic parsing with the smarts of an LLM for refinement.</p> <p>In conclusion, custom LLM vision models are an emerging method to convert documents. They offer unparalleled flexibility (any format to any output if you can describe it) and handle images/handwriting better than many static models. But they carry risks of errors and require careful implementation. They may be most useful when other tools fail (e.g., very messy documents, mixed languages, or needing AI to infer structure). For most standard use cases, a combination of classical parsing and targeted LLM use is preferable to relying fully on an LLM for conversion.</p>"},{"location":"blog/parsing-docs/#commercial-platforms","title":"Commercial Platforms","text":"<p>Open-source solutions can be powerful, but for enterprise-scale or highly accurate OCR needs, commercial cloud platforms are often considered. As one commenter aptly put it, \u201cIf this is for anything slightly commercial... you are probably going to have the best luck using Textract / Document Intelligence / Document AI. Nothing else will get everything out with high accuracy.\u201d. These cloud services benefit from powerful infrastructure and training on diverse real-world documents. We\u2019ll examine Amazon Textract and Azure Form Recognizer (Document Intelligence), with notes on Google\u2019s Document AI. While these services don\u2019t output Markdown directly, they provide structured data (JSON/XML) that can be transformed into Markdown.</p>"},{"location":"blog/parsing-docs/#amazon-aws-textract","title":"Amazon AWS Textract","text":"<p>AWS Textract is a fully managed service for document text extraction and analysis. It can handle printed text, handwriting, forms, and tables. Textract\u2019s API has two main modes: Detect Document Text (basic OCR) and Analyze Document (which identifies structure like form fields and table cells). For our purposes, using Analyze Document (with the \u201cTABLES\u201d feature) is most relevant, as it will yield table structures that we can convert to Markdown tables. Textract returns a JSON with blocks: lines, words, table cells, etc., including their coordinates on the page.</p> <p>Markdown Fidelity: Textract itself doesn\u2019t know about Markdown, but it preserves a lot of structure information. You can reconstruct paragraphs from the lines and their geometry (though it won\u2019t label something as a \u201cheading\u201d or \u201ctitle\u201d \u2013 it just gives text and position). Tables come out as structured data (cells with row/column indices), which can be directly turned into a Markdown <code>| cell | cell |</code> table. It also detects selection elements (checkboxes) and form key-values, which could be formatted as lists or definition lists in Markdown. However, Textract does not preserve styles like bold or italic, and it does not inherently know section hierarchy. All text is essentially \u201cplain\u201d. One could infer headings if, say, a line is larger font or bold \u2013 but Textract doesn\u2019t provide font info in its JSON. So you might rely on heuristic (e.g., all-caps lines or lines centered on page might be titles). In terms of output cleanliness, Textract is very accurate in text content. It was reported to \u201cperform admirably on tasks Tesseract struggles with, like handwriting and rough documents\u201d. For example, messy scans, water-damaged pages, or faint text are handled much better by Textract\u2019s ML models than by open-source OCR. It\u2019s been used for extracting information from forms and even handwriting with good accuracy. The downside noted is language support: Textract lags in non-Latin scripts. It struggles with languages like Chinese or Arabic, which Azure and Google handle better. For primarily English and similar languages, Textract is very strong.</p> <p>OCR and Scans: This is Textract\u2019s specialty. It was designed to process scanned documents at scale. It can read multi-column layouts (though the JSON will give you lines with coordinates; you have to determine reading order yourself by sorting lines top-left to bottom-right). Many PDF parsing issues (like reading order confusion) are mitigated because Textract relies on vision \u2013 it sees text in the correct order as humans would. However, it might not explicitly group columns; you might need to detect that two sets of lines are in parallel columns via their x-coordinates. Textract also handles handwritten text (with the AnalyzeDocument API including \u201cforms\u201d or using the specialized handwriting feature). For example, it can extract cursive writing from a form and include that in the text output. This is something most open-source tools (unless using a separate OCR engine) cannot do reliably. So for scan heavy workflows, Textract is a top performer. The trade-off is cost and needing to call an API.</p> <p>Chunking: Textract will output data page by page (each page is a set of blocks). It doesn\u2019t chunk beyond that. But since it\u2019s not giving you one big text blob, you have the flexibility to create chunks. Typically, one would call Textract, get JSON for each page, and then process each page\u2019s content. You could output one Markdown file per page, or combine them. If combining, you\u2019d likely iterate in order and assemble Markdown, inserting <code>\\pagebreak</code> or a level-1 heading for each page if desired. Structural chunking (by sections) would require you to identify section headings from the text and split accordingly. Because Textract itself doesn\u2019t output heading markers, you have to impose your own logic. For instance, you might detect that a line with large text (if you have access to the document PDF, you could cross-reference with PDF styles or run a layout analysis similar to Docling\u2019s on top of Textract\u2019s text). Alternatively, one could feed Textract\u2019s raw text into an LLM to decide chunk boundaries. In summary, chunking customization is in the developer\u2019s hands when using Textract \u2013 it\u2019s flexible but not automatic.</p> <p>Integration: AWS provides SDKs for Textract in Python (boto3). It\u2019s straightforward to call <code>Textract.analyze_document(Document=file, FeatureTypes=[\"TABLES\",\"FORMS\"])</code> and get JSON. The ecosystem includes higher-level tools like Amazon Comprehend or Amazon Athena that can consume Textract output, but for Markdown conversion you\u2019d write a parser for the JSON. Many open-source projects exist that convert Textract JSON to CSV or readable text. Adapting one to Markdown is feasible. Textract can be integrated into LangChain as a tool (some community loaders use Textract for OCR). Since Textract is a cloud service, it requires AWS credentials and has usage costs. It\u2019s serverless and can scale to thousands of pages quickly (with asynchronous batch jobs available). For LLM pipelines, one might use Textract to preprocess a batch of documents overnight, then feed the results to an LLM as needed. The ease of use is moderate: initial setup is easy if you are familiar with AWS, but interpreting the JSON correctly requires some work. Textract\u2019s JSON is verbose (each word has a block, etc.), but focusing on higher-level blocks (lines and table cells) simplifies it.</p> <p>Scientific/Technical Docs: Textract can extract any text, but it has no inherent understanding of formulas or special formatting. An inline equation in a PDF likely appears as a bunch of symbols which Textract will output in sequence (possibly with errors if symbols are unusual). It won\u2019t identify it as an equation or give it in LaTeX. So, similar to others, math support is minimal except as raw text. For things like chemical structures or complex diagrams, Textract will not \u201cunderstand\u201d them \u2013 it might not even output anything if it\u2019s purely graphical (no text). It does identify figures if there\u2019s alt text or captions (the caption text will be read as text). One interesting aspect: Textract provides bounding box coordinates, so one could theoretically identify regions (if you know an area is a figure and want to label it). But that\u2019s a lot of custom work. For technical tables and charts that are actually tables of numbers, Textract usually performs well \u2013 it can capture large tables as structured data (unlike some simpler tools that might flatten them). So for extracting data tables from scientific reports, Textract is useful, and you can output those as Markdown tables easily.</p> <p>Ease &amp; Flexibility: Textract is easy if you accept its defaults and just need text and tables. It\u2019s less flexible than open-source solutions in that you can\u2019t easily modify how it works (it\u2019s a black-box API). If Textract makes an error in reading order or misses a piece of text, you can\u2019t tweak a parameter \u2013 you\u2019d have to preprocess the document (e.g., split it differently) or use a different service. It\u2019s also tied to AWS, which might be a pro (if you\u2019re already on AWS) or a con (if data residency or vendor lock-in is a concern).</p> <p>In terms of output conversion: to integrate Textract into a Markdown workflow, you\u2019ll likely write a script to translate Textract\u2019s JSON to Markdown. This can include: iterating pages, assembling lines (maybe join lines into paragraphs using spacing info), outputting tables by collecting cell texts by row. It\u2019s doable, and there might be libraries to help parse Textract output.</p>"},{"location":"blog/parsing-docs/#azure-form-recognizer-document-intelligence","title":"Azure Form Recognizer (Document Intelligence)","text":"<p>Azure Form Recognizer (recently also referred to as Azure Document Intelligence) is Microsoft\u2019s analogous service. It offers a Layout model (for general document text &amp; layout), a General Document model (which returns a structured representation of paragraphs, tables, etc.), and specialized form models. For comparing, the Layout/General Document model is relevant: it will detect text lines, tables, and even some semantic roles like titles.</p> <p>Markdown Fidelity: Azure\u2019s output (if using the REST API v4.0) is a JSON with a content structure. It will list paragraphs, with their text, and indicate if something is a heading (the newer models do have a notion of a title or heading role for text blocks). It also captures font size and style to some extent. For example, it might mark a text element as \u201cappearance: bold\u201d. This means one could translate that into Markdown (bold text or <code># Heading</code> if large and bold). Azure also returns tables with cells and even grid structure reconstructed (similar to Textract). In side-by-side tests, Azure\u2019s model was slightly better at preserving reading order for multi-column docs. It also supports more languages than Textract (including Arabic, Chinese, etc.), often with higher accuracy in those scripts. One test noted \u201cAzure performed admirably on all document sets,\u201d successfully extracting text from a multilingual, rough scan that included Chinese, where Textract struggled. Azure even pulled out hidden OCR text in a PDF that had a hidden layer \u2013 showing it\u2019s quite robust. For Markdown conversion, Azure\u2019s advantage is the structured output: it essentially gives you the document in a structured tree (pages -&gt; paragraphs, tables, etc). This can be mapped to Markdown fairly directly (paragraphs to plain text, headings to <code>#</code> if identified, tables to Markdown tables).</p> <p>OCR and Scanned Support: Being a cloud OCR, Azure handles scanned and photographed documents very well. It also has a specific feature for handwritten text, particularly in forms or on lines. Azure\u2019s handwriting recognition is on par with or better than Textract\u2019s. And for non-Latin scripts or mixed-language docs, Azure is known to be strong. For example, an Arabic PDF test showed Azure correctly extracting right-to-left text with proper order, whereas an open-source approach might mess up character order. Azure\u2019s Form Recognizer has a further ability: you can train custom models for specific document types (beyond our scope, but useful if you wanted to extract certain fields from say lab reports).</p> <p>Chunking: Azure\u2019s service returns results per page as well, but since it groups lines into paragraphs, you could use those groupings as initial chunks. The \u201cGeneral Document\u201d model outputs elements that might correspond roughly to paragraphs or sections. If it tags headings, you can split by heading changes. Similar to Textract, you have control after the fact. Azure doesn\u2019t impose a chunking\u2014 it just gives a structured representation. But because it is aware of some structure (unlike Textract which just gives positions), you might more easily identify logical chunks. For instance, Azure might return an array of paragraphs in reading order. You could simply iterate and concatenate until you hit a new section (if a heading element is encountered, you start a new chunk). In practice, one could transform Azure\u2019s JSON to a Markdown with the same layout flow as the original document.</p> <p>Integration: Azure provides an SDK (<code>azure-ai-formrecognizer</code> for Python) and a REST API. You submit the document (or a URL to it) and get back the JSON result. Like Textract, it\u2019s asynchronous for large files (you submit a job and poll for result). The integration into Python pipelines is good, especially with the SDK which can directly give you Python objects for paragraphs, tables, etc. Converting to Markdown would be a custom step but straightforward given the structure. Azure\u2019s pricing is comparable to Textract\u2019s (roughly $1.5 per 1000 pages for layout analysis, with some free tier). If using within a large MS ecosystem, it can tie into Power Automate or Logic Apps too, but for LLM workflows, you\u2019d likely just use the API then feed the text to an LLM.</p> <p>Scientific/Technical support: Azure\u2019s OCR will capture text and numbers accurately, even in equations (though, again, just as text). It might not know an equation is an equation, but if the characters are clear, it will get them (and since it supports more Unicode, it might handle Greek symbols, etc., better than Textract). Azure also has a prebuilt model for PDFs that tries to identify references, footnotes, etc., if I recall correctly (or at least the general model might label footnote areas differently). This could help with complex academic PDFs by separating main text from footnotes. Tables with merged cells or spanning columns are often well-captured by Azure\u2019s table understanding. It might output complex tables more cleanly. For images/diagrams, like others, Azure won\u2019t interpret them (unless you use their Computer Vision API separately to caption an image \u2013 a possibility if one wants to incorporate that).</p> <p>Ease of Use: Azure\u2019s Form Recognizer is relatively easy to use via their studio UI or SDK, but from a pure code standpoint, it\u2019s a bit more involved than Textract\u2019s single API call because you have to create a client and possibly handle polling. The documentation is good, and there\u2019s an active community around it. One benefit is Azure\u2019s JSON is less cumbersome than Textract\u2019s. Also, Azure has recently unified their AI Document APIs, making it easier to call a single \u201canalyze document\u201d with the general model ID. This simplification helps developers. Flexibility is moderate: you can pick different models (Layout vs General vs specific). If the general model isn\u2019t perfect, you might try the Layout model which just gives lines and bounding boxes (similar to Textract output), or train a model. That said, training custom models is overkill just for Markdown conversion \u2013 the prebuilt general model is usually enough.</p>"},{"location":"blog/parsing-docs/#google-document-ai-and-others","title":"Google Document AI and Others","text":"<p>Google\u2019s Document AI is another major service, offering OCR and specialized parsers. Its OCR (Vision API Text Detection or Document OCR) is very powerful for multilingual and tricky layouts. It returns JSON similar to others, including structure. In a comparison, Google performed \u201cadmirably on all document sets\u201d and supported many languages (it handled the multilingual test with handwriting as well). It also has a high limit on cheap pricing for large volumes. The output from Google\u2019s Document OCR includes paragraphs and detected layout structure (Google\u2019s API uses a <code>Document</code> object with pages, blocks, paragraphs, etc., and it even can output a rendered HTML approximation). This makes it quite feasible to produce Markdown. For example, Google\u2019s PDF OCR often includes detected lists and tables as HTML in their Document AI output.</p> <p>Other notable tools include Adobe PDF Extract API \u2013 which is a commercial API by Adobe to extract PDF content with styling. It produces JSON and optionally HTML of the PDF content with styling and structure. That could be converted to Markdown and might preserve bold/italic and such (since Adobe\u2019s output is very detailed, including font information). Adobe\u2019s is a paid service and not as commonly used in LLM workflows yet, but it\u2019s relevant for completeness.</p> <p>There are also specialized tools like Mathpix (Snip) which is a service geared towards converting scientific PDFs (especially ones with lots of math) into LaTeX, Markdown, or HTML. Mathpix excels at extracting math equations accurately as LaTeX and can output Markdown with equations in <code>$...$</code>. It\u2019s a commercial tool often used by academics. If the priority is math fidelity, Mathpix is likely the best (but it won\u2019t preserve the full layout like a general tool; it\u2019s more about the content and LaTeX).</p> <p>Finally, we should mention LLMWhisperer (from the Unstract blog comparison) as a rising alternative. It combines OCR and LLM techniques to parse documents. It was shown to handle handwriting and complex forms better than Docling, but it\u2019s a relatively new/lesser-known solution (possibly closed-source or limited release). Its approach underscores the trend of hybrid solutions: using deep learning for OCR plus an LLM for contextual understanding.</p> <p>For most users, AWS, Azure, or Google\u2019s services will cover the bases when open-source isn\u2019t enough. They offer reliability and support, at the cost of direct Markdown output (requiring a conversion step) and of course cost per use.</p>"},{"location":"blog/parsing-docs/#comparative-features-and-performance","title":"Comparative Features and Performance","text":"<p>The following table summarizes the key features of each discussed tool/platform, to help compare their strengths:</p> Tool/Platform Markdown Fidelity OCR &amp; Scans Chunking Support Integration (Python/LLM) Scientific/Tech Support Ease of Use &amp; Flexibility IBM Docling (open) Excellent layout preservation \u2013 outputs headings, lists, tables, and images in Markdown as they appear. Bold/italics preservation is implicit. Lacks native math recognition (equations as images or text); future support planned. Avoids OCR for digital text (no errors from OCR). Uses integrated OCR (EasyOCR/Tesseract) for scanned content, which works but slower; can integrate faster OCR (RapidOCR). Struggles with handwriting or very poor scans. Outputs a structured Markdown document with page breaks and section headings intact. Can split by page or section via API options. Pairs with LlamaIndex/LangChain chunkers easily (e.g., each heading becomes a chunk). Structural chunking is natural due to preserved hierarchy. Native Python library + CLI. Seamless integration with LlamaIndex and LangChain. Local execution (no cloud needed). Requires downloading ML models; benefits from GPU for speed. Strong community support (IBM + open-source). Designed on manuals/reports \u2013 handles complex layouts, technical tables, and captions well. Does not yet parse formulas into LaTeX (will include as images or text). Good for multi-column scientific docs and preserving figure placement. Moderate setup (pip install + model downloads). 5-line usage for basic conversion. High resource usage (heavy models). Very flexible (MIT license, extensible models). Fast given its complexity, but not real-time lightweight. Unstructured.io (open) High structural fidelity. Identifies document elements (titles, paragraphs, lists, tables) and can output them in order. Can produce HTML/Markdown representations; tables preserved (as HTML or Markdown). Does not automatically mark bold/italic. Images extracted as base64 (user inserts into MD). Uses PDF text layer when available; falls back to OCR automatically for images. Default OCR via Tesseract or similar \u2013 adequate for moderate scans. Handwriting support limited in open version; Platinum cloud tier handles handwriting. Multi-language OCR depends on Tesseract models (cloud uses Google Vision for advanced). Strong chunking capabilities using semantic elements. By default, combines elements into chunks without breaking them \u2013 ensuring semantic coherence. Offers parameters to chunk by size or section breaks. Easy to get list of chunked elements for direct LLM input. Python library and REST API available. Easy integration with LangChain (already has UnstructuredLoader) and can be used in any Python workflow. Cloud API for scalability. Output is in Python object or JSON form (requires a small conversion to Markdown syntax). Well-suited for technical docs: preserves sections, can extract tables to structured data (keeping rows/columns). No special math formula parsing, but will capture formula text or images. Handles images/figures by extraction (not interpretation). Struggles only if documents are extremely unstructured (e.g., freeform handwriting). High flexibility (open source, self-host or cloud). Setup involves installing dependencies (which can be heavy if enabling all file types). Once running, it\u2019s straightforward. The cloud service simplifies use (just an API call). Good documentation. Need to write glue code to assemble final Markdown from elements (but many examples exist). LlamaIndex (open) +LlamaParse (cloud) Fidelity varies with the loader used: Using Docling or Unstructured connectors yields the same high fidelity outputs described above. Default PDF loader (PyMuPDF) preserves text but loses images and formatting (plain text only). LlamaParse cloud produces very good Markdown (headings, lists, tables, images), but some layout errors (column mix-ups, occasional missed heading) were observed. Tables from LlamaParse are accurately captured (Excellent). Depends on chosen backend: PyMuPDF loader has no OCR (scanned docs are not read). Unstructured/Docling backends add OCR support as those tools do. LlamaParse cloud likely uses OCR internally for scans \u2013 in tests it processed scanned content, but exact accuracy is proprietary. For robust OCR in LlamaIndex, one can call an external OCR tool or use the above libraries. Flexible chunking through LlamaIndex\u2019s <code>Document</code> and <code>Node</code> classes. Typically chunks by \\~512 tokens, respecting paragraph boundaries. With structured inputs (like Docling\u2019s Markdown), it can chunk by heading or semantic unit using the MarkdownNodeParser. Supports both structural (by sections/heads) and semantic (by embedding similarity or custom rules) chunking. Users can customize chunk size/overlap easily. Python integration is native \u2013 LlamaIndex is itself a Python framework for LLM data. Readers exist for various formats (and can be extended). It integrates with LangChain or can operate standalone for QA systems. LlamaParse cloud integration requires API usage or their web UI. Ecosystem is strong, with many community extensions. With the right parser, it handles scientific docs well (since it can ingest Docling output including tables, figures). No inherent scientific parsing on its own \u2013 relies on upstream conversion. It adds value by allowing metadata tagging (e.g., section titles as metadata) which can help in technical QA. LlamaParse aims to handle diagrams, etc., but likely doesn\u2019t interpret them beyond embedding the image reference. Open-source LlamaIndex is easy to install and use for basic tasks. Using advanced readers like DoclingReader adds installation steps (installing that plugin and Docling itself). Overall, developer-friendly with tutorials available. The cloud LlamaParse is easy (just upload docs), but cost can be high for large volumes. LlamaIndex code is very flexible \u2013 you can swap components (parsers, chunkers, vector stores) to suit your needs. Custom LLM Vision (GPT-4 Vision, etc.) Potentially high fidelity if prompted well, but not guaranteed. Can preserve structure and convert to Markdown with guidance. May even describe images/graphs in alt-text. However, risk of minor errors or rephrasing \u2013 not a verbatim conversion. Could produce LaTeX for equations (if model is capable), yielding better math fidelity than others. Quality depends on model\u2019s understanding. No built-in consistency check (requires manual QA). Excels at OCR on the fly: a strong multimodal LLM can read printed or handwritten text from images, even complex layouts. For example, GPT-4 Vision can read cursive handwriting or low-quality scans that stump normal OCR. Multi-language is typically supported if the model was trained on them. Essentially unlimited to what the model \u201csees\u201d \u2013 but slower and expensive for large docs. No automatic chunking; the user must feed manageable chunks (e.g., one page per prompt). An LLM could be instructed to output each section separately, but context window limits apply. If using GPT-4 (with vision), you might process page by page (each page yields Markdown). Combining results is on the user. Newer models with huge context (e.g., Claude 100k with images, or future multimodal with &gt;100 page context) could one-shot chunk a whole doc. Integration is non-trivial. Requires access to a vision-capable LLM (OpenAI API or local model with suitable interface). In Python, one might use OpenAI\u2019s API (once vision is available there) or HuggingFace transformers for open models. This approach is more custom \u2013 not a drop-in library. Some frameworks (LangChain) allow tools for vision, but prompt engineering is needed to ensure correct Markdown output. Could be very good for scientific content: can read equations and possibly output them in proper notation, interpret unusual symbols, and incorporate domain knowledge to correct OCR errors. Also can summarize or label figures. However, it might hallucinate interpretations (need to constrain it to just transcribe). It\u2019s the only approach that might add value by explaining diagrams or decoding complex handwritten notes using language understanding. Low ease-of-use at present. It\u2019s essentially an R\\&amp;D approach. Very flexible in theory (just change the prompt to get different output formats), but results can vary run to run. Also, using closed models raises data privacy issues and cost concerns. Open-source vision LLMs are improving, but require ML expertise to deploy. This method is best if other tools fail and you need an AI\u2019s understanding; otherwise, it\u2019s an expensive route to get Markdown. AWS Textract (cloud) High accuracy text extraction. Preserves tables and forms structure in JSON (can convert to MD tables/lists). Does not preserve visual formatting like headings or bold \u2013 all text is plain, position data can be used to infer structure. No native Markdown output. Excellent OCR, including difficult scans and handwriting. Struggles with non-Latin scripts (limited support for e.g. Chinese, Arabic). Very reliable on printed English, even noisy documents. Outputs per-page blocks; no direct chunking beyond page division. Custom logic needed to group lines into paragraphs/sections for chunks. Flexible because you have full control: e.g., chunk by page or combine small pages, etc. API/SDK integration (boto3). Easy to call from Python and get results. Integrates with other AWS AI services. Need to write parsing of JSON to Markdown. Fully managed (auto-scales, but internet required). Handles standard technical content well (text and tables). Mathematical notation will be captured as text but not formatted. No understanding of figures beyond extracting any text in them. Good for forms and structured reports. Using the service is easy (no ML setup), but processing output is a developer task. Moderate flexibility (you cannot change how Textract works, but you can post-process its results arbitrarily). Must consider cost ($) and possibly rate limits. Reliable and maintenance-free from an infrastructure perspective. Azure Form Recognizer (cloud) Very good structure preservation. Returns hierarchical content: paragraphs (with possible role like title), tables, etc. Allows detection of headings (if text is larger/bold) which can translate to Markdown headings. Like Textract, provides text content without markdown syntax, but with more layout info (e.g., list vs paragraph distinction). Top-tier OCR capability. Reads multi-language, multi-direction text better than most. Great on scanned docs, including mixed languages and moderate handwriting. Also captures things like checkbox state or selection marks. Also paginated output. Maintains grouping of lines into paragraphs, so easier to chunk by paragraph. Can identify section breaks to some degree. No auto-chunk output, but the structured result makes it simpler to define chunks (e.g., treat each paragraph or section as a chunk node). Strong Python SDK and REST API. Quick integration into applications. Output parsing is simpler than Textract due to labeled structure. Azure\u2019s ecosystem allows connecting to cognitive search, etc., if needed. Cloud-only (requires Azure account). Excels in varied content: from technical diagrams (extracts any text) to dense research papers. Known to successfully extract right-to-left text and mix of scripts in one doc. Math is treated as text. Overall, very suitable for technical documents and forms. Good documentation and relatively easy setup via Azure Portal or code. Some learning curve for JSON schema. More flexible than Textract in choosing different models (layout vs general). Pricing comparable to AWS. As a cloud service, very easy to scale and use once set up. <p>Table Key: For each tool, we highlight key points on Markdown output, OCR capability, chunking method, integration, scientific doc handling, and overall ease/flexibility.</p>"},{"location":"blog/parsing-docs/#conclusion","title":"Conclusion","text":"<p>Choosing the \u201cbest\u201d document-to-Markdown conversion tool depends on the specific needs and constraints of your project. If maximum fidelity of formatting (including integrated images and tables) is required and you prefer an open-source, on-premise solution, IBM Docling currently stands out \u2013 it produces highly structured Markdown output and integrates well with Python pipelines, though at the cost of heavy resource usage. Unstructured.io is another excellent open solution, especially valued for its chunking and semantic partitioning; it may require a bit more assembly to get a single Markdown file, but offers flexibility and solid performance (and a hosted option for tougher OCR tasks). For those already leveraging LLM frameworks, LlamaIndex provides a convenient way to plug these conversion capabilities into an end-to-end pipeline; with the right readers (Docling or Unstructured), it can match their fidelity and adds easy chunking and querying on top.</p> <p>When dealing with a lot of scanned documents or diverse languages where accuracy is paramount, the commercial OCR platforms (AWS Textract, Azure Form Recognizer, Google Doc AI) are very strong. Azure and Google especially handle multilingual and complex layouts with high accuracy. They can be used to get a baseline structured output, which can then be converted to Markdown. The downside is extra coding to transform JSON to Markdown and reliance on cloud services (with associated costs). If using these, a typical workflow might be: run the doc through the cloud OCR -&gt; get JSON -&gt; use a script or an LLM to format that into Markdown (the latter could be a clever use of an LLM for just formatting tasks).</p> <p>Custom LLM vision approaches are emerging as a powerful but experimental route. They might not yet replace dedicated conversion pipelines for reliability, but they offer a glimpse of the future: where you can simply \u201cask\u201d an AI to read a document and output structured Markdown, combining OCR, understanding, and formatting in one step. Already, some have found that a GPT-4 with vision can simplify workflows that previously required multiple OCR and parsing steps. Over time, as context lengths grow and multimodal models improve, this approach might become more feasible at scale (especially once you can process long documents in one go with an AI model). For now, it can be used in niche cases or to handle elements others can\u2019t (like describing a chart).</p> <p>In summary, for converting complex scientific or technical documents to Markdown for LLM consumption:</p> <ul> <li>IBM Docling is ideal when you need a local tool that outputs a near replica of the document\u2019s structure in Markdown (great for manuals, reports, etc., with minimal post-editing).</li> <li>Unstructured.io is excellent for breaking down documents into clean chunks and covers many formats; it\u2019s a good generalist and integrates easily, though you might have to assemble the final Markdown.</li> <li>LlamaIndex is a connector that can incorporate these tools \u2013 use it if you want an all-in-one pipeline to ingest docs, chunk, and query, but know that it inherits the strengths/weaknesses of whichever parser you choose (or try their LlamaParse for a managed solution with pretty high quality output).</li> <li>LLM vision models are a cutting-edge alternative when others fail \u2013 for instance, reading messy handwriting or extracting text from complex visuals \u2013 but require careful prompting and validation.</li> <li>AWS/Azure/Google cloud services are robust choices for OCR-intensive needs and large-scale operations, ensuring you don\u2019t miss any text. They give you the raw materials (text, structure) to build Markdown. Azure in particular provides rich structural info to identify headings and tables for Markdown conversion.</li> </ul> <p>Each option has trade-offs in terms of accuracy, completeness, cost, and convenience. In practice, many solutions combine these tools: e.g., using Docling to parse most of a PDF but falling back to an OCR API for a particularly problematic page, or using Unstructured to get chunks and then employing an LLM to polish the Markdown (adding backticks for code or formatting math). By understanding the strengths of each, one can pick the right tool for each aspect of the document conversion task.</p> <p>The good news is that the ecosystem is rapidly evolving \u2013 the focus on RAG and custom model training has driven vast improvements in document parsing in the last 1-2 years. We now have open libraries that rival proprietary systems and a convergence of techniques (CV for layout, NLP for content). For the foreseeable future, a combination of specialized parsing models (to avoid hallucinations and capture exact text) and LLM-based enhancements (for intelligent structuring and filling gaps) will likely offer the best results. Users should choose a tool or combination that best fits their documents\u2019 nature and their pipeline requirements. With the above comparisons, one can confidently select and implement a solution that yields clean, structured Markdown ready for LLM processing, even from the most complex documents.</p> <p>Sources:</p> <ol> <li>IBM Research Blog \u2013 \u201cA new tool to unlock data from enterprise documents for generative AI\u201d (Docling announcement)</li> <li>LlamaIndex Documentation \u2013 Docling Reader Overview</li> <li>Medium \u2013 \u201cUnleashing the Power of Your Data: RAG with Docling and LlamaIndex\u201d</li> <li>Unstructured.io Documentation \u2013 Chunking in Unstructured</li> <li>Level Up Coding \u2013 \u201cBest PDF Extractor for RAG? (LlamaParse vs Unstructured vs Vectorize)\u201d</li> <li>Unstract Blog \u2013 \u201cDocling vs LLMWhisperer\u201d (Docling features/limits)</li> <li>HackerNews Discussion \u2013 Ask HN: Parsing PDFs for RAG (comments on Unstructured, Textract, etc.)</li> <li>MuckRock \u2013 \u201cOur search for the best OCR tool in 2023\u201d (OCR tools comparison, Textract/Azure/Google results)</li> <li>Medium \u2013 \u201cUsing Docling\u2019s OCR with RapidOCR\u201d (Docling technical report excerpts)</li> <li>Level Up Coding \u2013 (table extraction quality in LlamaParse vs others)</li> </ol>"},{"location":"blog/pgadmin/","title":"Configuring pgAdmin for PostgreSQL","text":"<p>Date: April 27, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>pgAdmin is an Open Source administration and development platform for PostgreSQL -- Which claims to be the most advanced Open Source database in the world.  </p> <p>I don't know about that claim, but PostgreSQL is pretty great, it does have plugins for embedding vectors and vector indexing.  So I am currently a fan :)</p> <p>This is how I installed pgAdmin on Windows</p> <ol> <li>Downloaded and installed pgAdmin from their website</li> <li>run pgadmin4</li> <li>right click on \"servers\" and click \"register\" -&gt; \"server\"</li> <li>enter the following information:<ul> <li>General:<ul> <li>Name: {name of the instance of the database}</li> </ul> </li> <li>Connection:<ul> <li>Host: {host of the database} -- if local it will be 127.0.0.1</li> <li>Port: {port of the database} -- default is 5432, but this is often already used, so could create a new port number</li> <li>Maintenance database: {name of the database} -- if local it will be postgres</li> <li>Username: {username of the database} -- if local it will be postgres</li> <li>Password: {password of the database} -- if local it will be postgres</li> </ul> </li> <li>Save</li> </ul> </li> </ol> <p>At this point the connect will either be made or if there is an error you should follow the errors to fix the issue.  </p>"},{"location":"blog/scripts-and-modules/","title":"Scripts and modules","text":""},{"location":"blog/scripts-and-modules/#option-1-run-a-module-as-the-main-program","title":"Option 1:  run a module as the main program","text":"<ul> <li>remember tha a file contining python code used by other parts of a package is a \"module\" even if it only has one line of code in it</li> <li>The code must be in the directory tree of the projects and this must be executed from the project root directory</li> <li>this will execute all of the code in that module, and is a good way to execute <code>__main__</code> code</li> </ul> <p>To execute this you want to use the following syntax <pre><code>python -m modulepath.modulename\n</code></pre></p> <ul> <li>the <code>-m</code> means that it is searching for a module</li> <li>we give dots - <code>.</code> notatiion as this is how we define the paths as in an import statement.  </li> <li>there is no extension <code>.py</code> as we are refereing to it by its import path, not its filename</li> </ul>"},{"location":"blog/scripts-and-modules/#option-2-execute-a-standalone-script","title":"Option 2: Execute a standalone script","text":"<p><pre><code>python run_example_directly.py\n</code></pre> In this case the code is not part of the package, and the code resides outside of the package.  </p>"},{"location":"blog/uv/","title":"Switching to uv for Package and Environment Management","text":"<p>Date: May 11, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>Using pip as a package manager together with venv to manage isolated python environments has historically been the default way to manage packages and local environments.  However, there are a few drawbacks. It is slow for installing large packages, and the requirements files include all of the sub-dependencies of each package.  </p> <p>As a best practice it is ideal to have only the high-level packages in the installation, and to let those packages manage the installation of the sub-dependencies.  </p> <p>uv is now gaining popularity as a package and environemnt manager, combinding the functions of both pip and venv. I am jumping on the bandwagon, especially after waiting over 5 minutes for the <code>docling</code> installation which includes pytorch and hugging face models.  The same installation only takes seconds with uv.  </p>"},{"location":"blog/uv/#venv","title":"venv:","text":"<ul> <li>Creates isolated Python environments with their own packages</li> <li>Doesn't install packages itself</li> <li>Included in Python's standard library since 3.3</li> </ul>"},{"location":"blog/uv/#pip","title":"pip:","text":"<ul> <li>The standard Python package manager</li> <li>Installs, upgrades, and manages Python packages</li> <li>Used within virtual environments created by venv</li> </ul>"},{"location":"blog/uv/#uv-is-a-newer-alternative-that-combines-both-functions","title":"uv is a newer alternative that combines both functions:","text":"<ul> <li>Much faster package installer than pip (written in Rust)</li> <li>Also creates virtual environments like venv</li> <li>Often 10-100x faster than pip for installation operations</li> <li>Maintains compatibility with pip's ecosystem (requirements.txt, etc.)</li> </ul>"},{"location":"blog/uv/#installation-of-uv","title":"Installation of uv","text":"<p>I followed the blog post by Dave Ebbelaar to set up uv on my WSL2 linux environment on windows.  </p> <p>on macOS (using Homebrew) <pre><code>brew install uv\n</code></pre></p> <p>On Linux / macOS / WSL (using curl) <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> <p>On Windows (using PowerShell) <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p>"},{"location":"blog/uv/#creating-a-virtual-environment-using-uv","title":"Creating a virtual environment using uv","text":"<p>run the following command <pre><code>uv venv\n</code></pre></p> <p>This will create a folder named <code>.venv</code>.  To switch to this environment</p> <pre><code>source .venv/bin/activate \n</code></pre> <p>On Windows this would be: <pre><code>.venv\\Scripts\\activate\n</code></pre></p> <p>If you now type: <pre><code>which python\n</code></pre></p> <p>You will be able to verify that you are now working in the local .venv environment.</p>"},{"location":"blog/uv/#building-the-pyprojecttoml-file","title":"Building the <code>pyproject.toml</code> file","text":"<p>The requirements that a project needs are kept in a <code>.toml</code> file in the root directoy with the name <code>pyproject.toml</code> </p> <p>Below is an example of such a file for an example <code>nanobot</code> project.  There are many other things that can be included in this file, but these are the basics.</p> <pre><code>[project]\nname = \"nanobot\"\nversion = \"0.1.0\"\ndescription = \"NanoBot - AI assistant application\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n    # Database\n    \"psycopg2-binary\",\n\n    # Configuration\n    \"pydantic-settings\",\n    \"python-dotenv\",\n\n    # Template Engine\n    \"jinja2\",\n\n    # Logging\n    \"logfire\",\n\n    # Data Processing\n    \"pandas\",\n    \"matplotlib\",\n\n    # Document Processing\n    \"docling\",\n    \"docling-core\",\n\n    # OpenAI\n    \"openai\",\n\n    # Tokenization\n    \"tiktoken\",\n\n    # Web UI\n    \"streamlit\",\n]\n</code></pre>"},{"location":"blog/uv/#installing-using-sync","title":"Installing using Sync","text":"<p>To install the dependencies in the <code>pyproject.toml</code> file, you will run the following command:</p> <pre><code>uv sync\n</code></pre> <p>You will then see the packages and their dependencies being installed, ofter at a speed of 10x-100x of what a standard pip installation would take. </p>"},{"location":"blog/uv/#addidng-packages-with-uv","title":"Addidng packages with uv","text":"<p>You will preceed many of your commands with <code>uv</code>.  ie for package installation:</p> <pre><code>uv add [package name(s)]\n</code></pre>"},{"location":"blog/uv/#uv-lock","title":"uv lock","text":"<p>This will allow you to lock the dependencies and versions.  Please look into the uv docs for more information. </p>"},{"location":"setup/00-index-setup/","title":"Getting Started with NanoBot","text":"<p>This guide will help you set up and start using NanoBot.</p>"},{"location":"setup/00-index-setup/#installation-prerequisites","title":"Installation Prerequisites","text":"<ul> <li>Python 3.10+ (I am using 3.12.x)</li> <li>PostgreSQL with pgvector extension in 1 of the following configurations:<ul> <li>Local Pastgres Installation</li> <li>Neon database URL (neon.tech)</li> </ul> </li> <li>OpenAI API key</li> <li>Logfire token (for logging LLM calls)</li> </ul>"},{"location":"setup/00-index-setup/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/worldlycode/nanobot-dev.git\ncd nanobot-dev\n</code></pre></p> </li> <li> <p>Create a virtual environment:    <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -r requirements.txt\n</code></pre>    the current <code>requirements.txt</code> file includes the following:    <pre><code># Database\npsycopg2-binary\n\n# Configuration\npydantic-settings\npython-dotenv\n\n# Template Engine\njinja2\n\n# Logging\nlogfire\n\n# Data Processing\npandas\nmatplotlib\n\n# Document Processing\ndocling\ndocling-core\n\n# OpenAI\nopenai\n\n# Tokenization\ntiktoken\n\n# Web UI\nstreamlit\n\n# if you use Jupyter notebooks\nipykernel\n\n#testing\npytest\n</code></pre>    Note this make take severl minute as the Docling library installs several ML models from hugging face, and also pytorch packages to run these locally</p> </li> </ol>"},{"location":"setup/00-index-setup/#setup","title":"Setup","text":"<p>See more details in the following setup files</p> <ol> <li> <p>Set up environment variables:  </p> <p>NOTE: This section is under development    Create a <code>.env</code> file with the following variables:    <pre><code>OPENAI_API_KEY=your_openai_api_key\nLOCAL_DB_NAME=nanobot\nLOCAL_DB_USER=postgres\nLOCAL_DB_PASSWORD=your_password\nLOCAL_DB_HOST=localhost\nLOCAL_DB_PORT=5432\nLOGFIRE_TOKEN=your_logfire_token\nNEON_URL=your_neon_db_url\nUSE_NEON=False (defaults to using local db)\n</code></pre></p> </li> <li> <p>Initialize the database:  </p> <p>NOTE: This section is under development    <pre><code>python -m app.database.db_setup\n</code></pre></p> </li> <li> <p>Run the test files:    Run the following command:    <pre><code>pytest tests\n</code></pre></p> </li> </ol>"},{"location":"setup/00-index-setup/#running-the-application","title":"Running the Application","text":"<p>Start the Streamlit web interface from the root directory:</p> <pre><code>streamlit run nanobot.py\n</code></pre> <p>This will open a web browser with the NanoBot interface.</p>"},{"location":"setup/database-table-setup/","title":"Initializing your Database","text":"<ul> <li>Toggle switch in your environment file or in <code>app.config.settings.py</code> to be local or Neon.  </li> <li>runn the code to set up the tables in the chosen location</li> <li>Import sample data</li> <li>Have a script that tests this sample data</li> </ul>"},{"location":"setup/database-table-setup/#todo","title":"TODO","text":"<p>Setup a json or a csv (to be read by pandas) with some sample data to test and load into the database for testing once configured .</p>"},{"location":"setup/env-config/","title":"Setting up the <code>.env</code> file","text":"<p>Create a <code>.env</code> file in the root directory with the following variables: <pre><code>OPENAI_API_KEY=your_openai_api_key\nLOCAL_DB_NAME=nanobot\nLOCAL_DB_USER=postgres\nLOCAL_DB_PASSWORD=your_password\nLOCAL_DB_HOST=localhost\nLOCAL_DB_PORT=5432\nLOGFIRE_TOKEN=your_logfire_token\nNEON_URL=your_neon_db_url\nUSE_NEON=False (defaults to using local db)\n</code></pre></p> <ul> <li> <p>If you do not have a Logfire token please see the Logfire-Setup Page</p> </li> <li> <p>If you do not have a Neon URL please see the Neon-Setup Page</p> </li> </ul>"},{"location":"setup/logfire-setup/","title":"Setting up Pydantic Logfire","text":"<p>Logfire by Pydantic is used to record the interaction between the LLM and the application.  Using Logfire one is able to see what is being passed to the model, and what the model is returning.  Additionally you can also see the length of timeeach call takes.  </p> <p>To Use the Nanobot-Dev codebase without modification you will need to configure a Logfire account.  The can be used at no charge for a ridicolous number of calls, (10 million free spans/metrics per month).  </p> <p>Follow these steps:</p> <ol> <li>Go to the main website at https://pydantic.dev/logfire and create an account</li> <li>Log into Logfire</li> <li>Go to the Projects Page and creeate a new project.  </li> <li>Generate a <code>write token</code> and save it for future use</li> <li>Copy the logfire token into your <code>.eve</code> file.  Do not use brackets or quotes or spaces.</li> </ol> <p><pre><code>LOGFIRE_TOKEN=[YOUR_TOKEN_HERE] \n</code></pre> This is what the project page looks like. </p> <p></p> <p>In order to see the live updates you will want to click on the <code>Live</code> tab at the top toolbar of the page.  Each time you make an LLM Call from your application, it will update here in the live view.  You can also search all history going up to 30 days back.</p> <p>A call in the live view looks like the following image.</p> <p></p>"},{"location":"setup/neon-setup/","title":"Configuring Neon (Neon.tech)","text":""},{"location":"setup/neon-setup/#account","title":"Account","text":"<p>You will need to set up an account.  There is a free tier which will be fine to get you started. You will need to verify the email address that you are using.  </p>"},{"location":"setup/neon-setup/#project","title":"Project","text":"<p>After verifying your email information, you will need to create a project.  </p> <ul> <li>Below is a screenshot where I have created a nanobot-dev project.</li> <li>You will pick a location that is near the area that this data will serve.    </li> </ul> <p></p>"},{"location":"setup/neon-setup/#database","title":"Database","text":"<p>You will see that your default database name is neondb.  This is fine to keep for now.  </p>"},{"location":"setup/neon-setup/#branches","title":"Branches","text":"<p>Both production and development braches are made.  Note that the development branch is a child of the main production branch.  The Default branch is the <code>production</code> branch.  </p>"},{"location":"setup/neon-setup/#connecting-to-the-database-url","title":"Connecting to the database (URL)","text":"<p>Clicking on the production branch will bring up the following screen</p> <p></p> <p>You then want to click onn the <code>Connect</code> button on the far right which will open a screen that looks like the one below:</p> <p></p> <p>Copy the connection string.  This is the <code>NEON_URL</code> that you will need in your <code>.env</code> file to connect to the database</p>"},{"location":"setup/postgres-local-setup/","title":"Local PosgreSQL Setup","text":"<p>This is a total work under construction -- </p> <p>Maybe should just refer people to setup pages for their particular OS:</p> <ul> <li>Mac-OS</li> <li>LInux</li> <li>WIndows</li> <li>WSL (Windows subsystem on linux)</li> </ul> <p>In theory, if they want to serve this as an application, they should either deploy through a Docker VM or Neon, or similar cloud instance...</p>"},{"location":"setup/running-nanobot/","title":"Running the Nanobot Streamlit Application","text":"<p>from the root directory: <pre><code>streamlit run nanobot.py\n</code></pre></p>"},{"location":"user-guide/00-index-user-guide/","title":"Nanobot Overview","text":""},{"location":"user-guide/00-index-user-guide/#architecture","title":"Architecture","text":"<p>Nanobot is a RAG (Retrieval-Augmented Generation) system with a layered architecture:</p> <ol> <li>Presentation Layer: Streamlit UI (<code>nanobot.py</code>)</li> <li>Service Layer: Core business logic (<code>app/services/</code>)</li> <li>Data Access Layer: Database interactions (<code>app/database/</code>)</li> <li>Utility Layer: Helper functions (<code>app/utils/</code>)</li> </ol>"},{"location":"user-guide/00-index-user-guide/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<p>Documents flow through the system in these stages:</p> <ol> <li>Document conversion (Docling)</li> <li>Text chunking with configurable strategies</li> <li>Embedding generation (OpenAI)</li> <li>Vector storage (PostgreSQL with pgvector)</li> <li>Similarity-based retrieval</li> <li>LLM integration for response generation</li> </ol>"},{"location":"user-guide/00-index-user-guide/#key-features","title":"Key Features","text":"<ul> <li>Modular codebase with clean separation of concerns</li> <li>Multiple document chunking strategies</li> <li>Vector similarity search</li> <li>Interactive Streamlit interface</li> <li>Enterprise-ready database with transaction support</li> <li>Extensible framework for AI integrations</li> </ul>"},{"location":"user-guide/00-index-user-guide/#core-components","title":"Core Components:","text":"<ol> <li>app/ - Main application code</li> <li>services/ - Service layer with document processing, chunking, and API integrations<ul> <li><code>document_service.py</code> - Complete pipeline for document processing</li> <li><code>chunking_service.py</code> - Document chunking strategies</li> <li><code>openai_service.py</code> - OpenAI API integration</li> <li><code>prompt_loader.py</code> - For loading prompt templates</li> </ul> </li> <li>database/ - Database interactions<ul> <li><code>setup.py</code> - Database initialization</li> <li><code>common.py</code> - Shared database utilities</li> <li><code>insert.py</code> - Database insertion operations</li> <li><code>retrieval.py</code> - Vector search and retrieval</li> <li><code>transaction.py</code> - Transaction management</li> <li><code>maintenance.py</code> - Database maintenance tasks</li> </ul> </li> <li>models/ - Data models and validators</li> <li>utils/ - Utility functions<ul> <li>File handling, tokenization, logging, etc.</li> </ul> </li> <li>config/ - Configuration settings</li> <li> <p>prompts/ - Prompt templates</p> </li> <li> <p>Main Scripts:</p> </li> <li><code>nanobot_poc.py</code> - Main Streamlit application with UI</li> <li><code>process_and_load.py</code> - CLI tool for processing documents</li> </ol>"},{"location":"user-guide/00-index-user-guide/#supporting-directories","title":"Supporting Directories:","text":"<ul> <li>data/ - Document storage</li> <li>tests/ - Comprehensive test suite</li> <li>examples/ - Example code </li> <li>sandbox/ - For experimentation and notebooks</li> <li>logs/ - Log files</li> </ul>"},{"location":"user-guide/document-chunking/","title":"Chunking Strategies","text":"<p>After conversion, documents are split into chunks using one of several chunking strategies. Each strategy is optimized for different use cases:</p> Strategy Description Best For default Standard chunking with moderate chunk size General purpose use balanced Balanced approach between context preservation and chunk size Most document types fine_grained Smaller chunks for more precise retrieval Technical documents, reference materials context Larger chunks that preserve more context Q&amp;A, summarization tasks hierarchical Chunks based on document's natural hierarchy Structured documents with clear sections <p>The chunking process is handled by the <code>ChunkingService</code> class:</p> <pre><code>from app.services.chunking_service import ChunkingService\n\n    # more code to go here\n</code></pre>"},{"location":"user-guide/document-conversion/","title":"Document Conversion and Chunking","text":"<p>This guide explains how NanoBot processes documents, from initial conversion to chunking and embedding.</p>"},{"location":"user-guide/document-conversion/#supported-document-types","title":"Supported Document Types","text":"<p>NanoBot supports various document types through its document conversion pipeline:</p> <ul> <li>PDF (.pdf): Full support for text extraction, including tables and basic formatting</li> <li>Microsoft Word (.docx): Support for text, tables, and document structure</li> <li>Text files (.txt): Plain text processing</li> <li>Web Pages (URLs): Extract content directly from web pages</li> </ul>"},{"location":"user-guide/document-conversion/#document-conversion","title":"Document Conversion","text":"<p>The first step in processing a document is converting it to a structured format that preserves the document's content and structure. This is handled by the <code>DocumentService</code> class using the Docling library.</p> <pre><code>from app.services.document_service import DocumentService\n\n# Initialize the service\ndocument_service = DocumentService()\n\n# Convert a document\nconverted_doc = document_service.convert_document(\"path/to/document.pdf\")\n</code></pre> <p>During conversion, NanoBot:  </p> <ul> <li>Extracts text content  </li> <li>Preserves document structure (headings, paragraphs)  </li> <li>Identifies metadata (title, author, etc.)  </li> <li>Optionally saves intermediate formats for debugging  </li> </ul>"},{"location":"user-guide/document-processor/","title":"Document Processor","text":""},{"location":"user-guide/document-processor/#overview","title":"Overview","text":"<p>The document processor is a versatile tool that processes documents and loads them into a database. It handles the entire pipeline from document extraction to chunking and database storage, with support for both local PostgreSQL and Neon database options.</p>"},{"location":"user-guide/document-processor/#features","title":"Features","text":"<ul> <li>Process a single document with configurable chunking strategies</li> <li>Batch process all documents in a specified directory</li> <li>Automatic database creation and initialization</li> <li>Support for multiple chunking strategies for different use cases</li> <li>Compatible with both local PostgreSQL and Neon cloud databases</li> </ul>"},{"location":"user-guide/document-processor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python environment with required dependencies</li> <li>PostgreSQL database or Neon database access</li> <li>OpenAI API key for document processing</li> <li>Environment configuration in a <code>.env</code> file</li> </ul>"},{"location":"user-guide/document-processor/#usage-instructions","title":"Usage Instructions","text":""},{"location":"user-guide/document-processor/#basic-commands","title":"Basic Commands","text":"<p>Process a single file with the default chunking strategy: <pre><code>python -m document_processor file.pdf\n</code></pre></p> <p>Process a single file with a specific chunking strategy: <pre><code>python -m document_processor file.pdf --strategy balanced\n</code></pre></p> <p>Process all files in the default directory with the default chunking strategy: <pre><code>python -m document_processor --all\n</code></pre></p> <p>Process all files with a specific chunking strategy: <pre><code>python -m document_processor --all --strategy fine_grained\n</code></pre></p>"},{"location":"user-guide/document-processor/#available-chunking-strategies","title":"Available Chunking Strategies","text":"<ul> <li>default: Standard chunking with moderate chunk size</li> <li>balanced: Balanced approach between context preservation and chunk size</li> <li>fine_grained: Smaller chunks for more precise retrieval</li> <li>paragraph: Chunk by paragraphs regardless of size</li> </ul>"},{"location":"user-guide/document-processor/#example-workflow","title":"Example Workflow","text":"<ol> <li>Configure your database and API settings in <code>.env</code></li> <li>Place your documents in the appropriate directory</li> <li>Run the document processor with your preferred strategy</li> <li>The tool will:</li> <li>Create a database if it doesn't exist (for local PostgreSQL)</li> <li>Initialize tables and extensions</li> <li>Process documents according to the selected strategy</li> <li>Store document chunks in the database</li> </ol>"},{"location":"user-guide/document-processor/#how-it-works","title":"How It Works","text":"<ol> <li>Database Setup: The tool checks if the specified database exists and creates it if necessary</li> <li>Document Processing: Documents are processed using the selected chunking strategy and OpenAI's embedding capabilities</li> <li>Data Storage: Processed document chunks are stored in the database with vector embeddings</li> <li>Reporting: The tool reports on the number of chunks created and stored</li> </ol>"},{"location":"user-guide/document-processor/#configuration","title":"Configuration","text":"<p>The document processor uses environment variables for configuration. Ensure your <code>.env</code> file contains the necessary parameters:</p>"},{"location":"user-guide/document-processor/#database-configuration","title":"Database Configuration","text":"<p>For local PostgreSQL:  </p> <ul> <li><code>LOCAL_DB_NAME</code>: Database name  </li> <li><code>LOCAL_DB_USER</code>: Database username  </li> <li><code>LOCAL_DB_PASSWORD</code>: Database password  </li> <li><code>LOCAL_DB_HOST</code>: Database host  </li> <li><code>LOCAL_DB_PORT</code>: Database port  </li> </ul> <p>For admin operations (creating databases):  </p> <ul> <li><code>LOCAL_DB_ADMIN_NAME</code>: Admin database name  </li> <li><code>LOCAL_DB_ADMIN_USER</code>: Admin username  </li> <li><code>LOCAL_DB_ADMIN_PASSWORD</code>: Admin password  </li> <li><code>LOCAL_DB_ADMIN_HOST</code>: Admin host  </li> <li><code>LOCAL_DB_ADMIN_PORT</code>: Admin port  </li> </ul> <p>For Neon database:  </p> <ul> <li><code>NEON_DB_URL</code>: Complete connection URL for Neon database  </li> <li><code>USE_NEON</code>: Set to \"true\" to use Neon instead of local PostgreSQL  </li> </ul> <p>For API Keys:  </p> <ul> <li><code>OPENAI_API_KEY</code>: Your OpenAI API key for document processing and embeddings  </li> <li><code>OPENAI_MODEL</code>: (Optional) The OpenAI model to use (defaults to \"gpt-4o\")  </li> </ul>"},{"location":"user-guide/document-processor/#switching-between-local-and-neon-databases","title":"Switching Between Local and Neon Databases","text":"<p>To use a Neon database instead of local PostgreSQL:  </p> <ol> <li>Ensure your <code>.env</code> file contains a valid <code>NEON_DB_URL</code> </li> <li>Set <code>USE_NEON=true</code> in your <code>.env</code> file  </li> </ol>"},{"location":"user-guide/document-processor/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter database connection issues:  </p> <ul> <li>Verify your database credentials in the <code>.env</code> file  </li> <li>Ensure PostgreSQL is running (for local database)  </li> <li>Check that the Neon database URL is correct (for Neon database)  </li> <li>Check that the user has appropriate permissions  </li> </ul> <p>For processing errors:  </p> <ul> <li>Verify that document files are accessible and not corrupted  </li> <li>Check that required Python dependencies are installed  </li> </ul> <p>For OpenAI API issues:  </p> <ul> <li>Verify your API key is correct  </li> <li>Check for rate limiting or quota issues  </li> <li>Ensure you have access to the specified models  </li> </ul>"},{"location":"user-guide/git/","title":"Git Collaboration","text":"<p>This is the first time I will be working with other in the same codebase, and thinking about how this actually will work.  This document is a space for us to update how this works best for us.  </p>"},{"location":"user-guide/git/#branches","title":"Branches","text":"<p>Currently cleaning up <code>clean-main</code> but will probably rename as <code>development</code></p>"},{"location":"user-guide/git/#how-to-work-on-your-own-version-of-the-code","title":"How to work on your own version of the code","text":"<p>There are various options  * create your own personal branch -- create a pull request in the development branch when your feature is tested * create a seperate directory called <code>/sandbox</code>.  Make certain your <code>.gitignore</code> file contains this folder so that it is not committed back to the repo before you are ready.  </p>"},{"location":"user-guide/git/#collaboration-guidelines","title":"\ud83e\udd1d Collaboration Guidelines","text":"<p>DISCLAIMER Below was written by ChatGPT.  I did ask it to be friendly and warm! :-D  </p>"},{"location":"user-guide/git/#working-together","title":"Working Together","text":"<p>To help us keep the project organized and high-quality, we follow a few lightweight practices:</p> <ul> <li>The <code>main</code> branch is protected to ensure stable and reliable code.</li> <li>Changes should come through Pull Requests (PRs) \u2014 it's the best way to review and improve together.</li> <li>Direct pushes to <code>main</code> are restricted to keep history clean and traceable.</li> </ul>"},{"location":"user-guide/git/#opening-pull-requests","title":"Opening Pull Requests","text":"<p>When opening a PR:</p> <ul> <li>Start from a feature branch.</li> <li>One peer approval is encouraged before merging.</li> <li>Please resolve any open review comments or conversations.</li> <li>If new commits are added after approval, a quick re-approval helps keep reviews up to date.</li> <li>Keep your branch in sync with <code>main</code> to avoid conflicts.</li> </ul>"},{"location":"user-guide/git/#merging-options","title":"Merging Options","text":"<p>You are free to choose the merge method that fits best:</p> <ul> <li>Merge, Squash, or Rebase are all supported.</li> <li>Squash merging is recommended for small or self-contained changes to keep commit history tidy.</li> </ul>"},{"location":"user-guide/git/#stewardship","title":"Stewardship","text":"<ul> <li>Admins help guide the project and can bypass branch protections if necessary.</li> <li>Day-to-day contributions, reviews, and merges are open and collaborative.</li> </ul>"},{"location":"user-guide/git/#best-practices","title":"\ud83e\uddf9 Best Practices","text":"<ul> <li>Name branches clearly (e.g., <code>feature/login-page</code>, <code>fix/readme-typo</code>).</li> <li>Try to keep PRs focused on a single topic or improvement.</li> <li>Include a short description in your PR:</li> <li>What was changed</li> <li>Why it was needed</li> <li>How it was tested</li> </ul>"},{"location":"user-guide/git/#quick-pull-request-template-optional","title":"\u2705 Quick Pull Request Template (Optional)","text":"<p>When opening a Pull Request, feel free to use:</p> <pre><code>## Summary\n- [What does this change do?]\n\n## Testing\n- [How was this verified?]\n\n## Review\n- [Anything specific you'd like reviewers to focus on?]\n</code></pre>"},{"location":"user-guide/git/#thank-you-for-contributing-and-helping-the-project-grow","title":"\ud83d\ude80 Thank you for contributing and helping the project grow!","text":""},{"location":"user-guide/settings/","title":"Configuration Guide: Using the Settings System","text":"<p>This guide explains how to configure the application through the settings system and environment variables.</p>"},{"location":"user-guide/settings/#overview","title":"Overview","text":"<p>Our application uses a configuration system based on Pydantic that allows for: - Sensible defaults for all settings - Configuration via environment variables or <code>.env</code> file - Type validation to prevent configuration errors</p>"},{"location":"user-guide/settings/#the-settings-module","title":"The Settings Module","text":"<p>The <code>app/config/settings.py</code> module defines all application settings in a structured way. Settings are organized into categories like database settings, API keys, file paths, etc.</p>"},{"location":"user-guide/settings/#using-the-env-file","title":"Using the <code>.env</code> File","text":"<p>The easiest way to configure the application is by creating a <code>.env</code> file in the project root. This file contains environment variables that override default settings.</p>"},{"location":"user-guide/settings/#basic-env-example","title":"Basic <code>.env</code> Example","text":"<pre><code># Database Configuration\n\nLOCAL_DB_ADMIN_NAME=postgres\nLOCAL_DB_ADMIN_USER=postgres\nLOCAL_DB_ADMIN_PASSWORD=postgres\nLOCAL_DB_ADMIN_HOST=localhost\nLOCAL_DB_ADMIN_PORT=5432\n\n\nLOCAL_DB_NAME=myapp\nLOCAL_DB_USER=postgres\nLOCAL_DB_PASSWORD=mysecretpassword\nLOCAL_DB_HOST=localhost\nLOCAL_DB_PORT=5432\n\n# OpenAI API Configuration\nOPENAI_API_KEY=sk-your-api-key\nLOGFIRE_TOKEN=pylf-your-token\n\n# Neon Database String\nNEON_DB_URL=postgresql://neondb_owner:full-neon-url\nUSE_NEON=true\n\n# File Paths (optional)\nDATA_BASE_DIR=data\nDATA_ORIGINAL_DOCS_DIR=original_docs\nDATA_CONVERTED_DOCS_DIR=converted_docs\nDATA_CHUNKING_RESULTS_DIR=chunking_results\n</code></pre>"},{"location":"user-guide/settings/#settings-categories","title":"Settings Categories","text":""},{"location":"user-guide/settings/#file-path-settings","title":"File Path Settings","text":"<p>Controls where the application stores and processes documents.</p> Setting Environment Variable Default Description Base Data Directory <code>DATA_BASE_DIR</code> <code>data</code> Root directory for all data files Original Documents <code>DATA_ORIGINAL_DOCS_DIR</code> <code>original_docs</code> Where uploaded documents are stored Converted Documents <code>DATA_CONVERTED_DOCS_DIR</code> <code>converted_docs</code> Where processed documents are stored Chunking Results <code>DATA_CHUNKING_RESULTS_DIR</code> <code>chunking_results</code> Where document chunks are stored"},{"location":"user-guide/settings/#database-settings","title":"Database Settings","text":"<p>Controls database connection parameters.</p> Setting Environment Variable Default Description Database Name <code>LOCAL_DB_NAME</code> Name of the database Database User <code>LOCAL_DB_USER</code> Database username Database Password <code>LOCAL_DB_PASSWORD</code> Database password Database Host <code>LOCAL_DB_HOST</code> Database server address Database Port <code>LOCAL_DB_PORT</code> Database server port"},{"location":"user-guide/settings/#openai-api-settings","title":"OpenAI API Settings","text":"<p>Controls OpenAI API integration.</p> Setting Environment Variable Default Description API Key <code>OPENAI_API_KEY</code> Your OpenAI API key Model <code>OPENAI_MODEL</code> <code>gpt-4o</code> Model to use for completions Embedding Model <code>OPENAI_EMBEDDING_MODEL</code> <code>text-embedding-3-small</code> Model for embeddings Temperature <code>OPENAI_TEMPERATURE</code> <code>0.0</code> Controls randomness in responses"},{"location":"user-guide/settings/#configuration-priority","title":"Configuration Priority","text":"<p>Settings are loaded in the following order of precedence (highest to lowest):</p> <ol> <li>Environment variables</li> <li>Variables in the <code>.env</code> file</li> <li>Default values from <code>settings.py</code></li> </ol> <p>This means that environment variables will always override <code>.env</code> file settings, which in turn override defaults.</p>"},{"location":"user-guide/settings/#accessing-settings-in-code","title":"Accessing Settings in Code","text":"<p>To use these settings in your code:</p> <pre><code>from app.config.settings import settings\n\n# Database connection parameters\ndb_params = settings.local_db.get_connection_dict()\n\n# File paths\noriginal_docs_path = settings.file_paths.get_original_docs_path()\nconverted_docs_path = settings.file_paths.get_converted_docs_path()\n\n# API Settings\nopenai_key = settings.openai.api_key\n</code></pre>"},{"location":"user-guide/settings/#directory-structure","title":"Directory Structure","text":"<p>When you first run the application, the system will automatically create all necessary data directories based on your settings. The default structure is:</p> <pre><code>project_root/\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 original_docs/\n    \u251c\u2500\u2500 converted_docs/\n    \u2514\u2500\u2500 chunking_results/\n</code></pre> <p>You can customize these locations using the environment variables listed above.</p>"},{"location":"user-guide/settings/#reloading-settings","title":"Reloading Settings","text":"<p>If you need to reload settings at runtime (after changing environment variables):</p> <pre><code>from app.config.settings import reload_settings\n\n# Reload all settings\nreload_settings()\n</code></pre> <p>This will recreate all required directories with the updated paths.</p>"},{"location":"user-guide/tests/","title":"Tests","text":"<p>Last updated 4/26/2025 by sam-i-am</p> <p>The importance of writing test code on an ongoing basis is becomming apparant to me.  This is especially so with the involvement of multiple contributors.  </p> <p>Obviously this test folder should only exist in the development code (as opposed to the deployed production code)</p>"},{"location":"user-guide/tests/#pytest","title":"Pytest","text":"<p>Currently we are using the <code>pytest</code> module for our tests.  As of the date of the writing of this post the test directory contains the following tests:</p> <pre><code>\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_database_retrieval.py\n\u2502   \u251c\u2500\u2500 test_config_settings.py\n\u2502   \u251c\u2500\u2500 test_transaction.py\n\u2502   \u251c\u2500\u2500 test_chunking_service.py\n\u2502   \u251c\u2500\u2500 test_error_handling.py\n\u2502   \u251c\u2500\u2500 test_document_service.py\n\u2502   \u251c\u2500\u2500 test_pydantic_validator.py\n\u2502   \u251c\u2500\u2500 __init__.py\n</code></pre> <p>This entire test code can be run from the root directory by running</p> <pre><code>pytest tests\n</code></pre> <p>To be rigerously honest, I asked Claude to generate most of these tests, though I have modified some of them.  But the idea is for every module that we write we should have some tests that continuously test the functionality of the module, and run this test code frequently, at least daily.  </p> <p>This will let us catch quickly where our code may have inadvertly gotten broken or had undesired effects and allow us to fix things quickly.  </p>"},{"location":"user-guide/utils/","title":"Utils (Utility function) Folder","text":"<p>April 24, 2025</p> <p>In <code>app/utils</code> directory there are several modules which handle some housekeeping tasks.  We describe these below</p>"},{"location":"user-guide/utils/#treepy","title":"<code>tree.py</code>","text":"<p>The <code>tree.py</code> utility draws a tree of the project or directory.  To run this from the root directory command line (from the <code>nanobot-poc</code> directory), entering</p> <pre><code>python -m app.utils.tree/utils\n</code></pre> <p>will print the following structure in the terminal</p> <pre><code>app/\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init.__.py\n\u2502   \u251c\u2500\u2500 json_formatter.py\n\u2502   \u251c\u2500\u2500 logger.py\n\u2502   \u251c\u2500\u2500 logging_examples.py\n\u2502   \u251c\u2500\u2500 tree.py\n\u2502   \u251c\u2500\u2500 file_handling.py\n\u2502   \u251c\u2500\u2500 tokenizer.py\n</code></pre> <p>To print the entire tree structure you would use:</p> <pre><code>python -m app.utils.tree\n</code></pre> <p>You can also add arguments as follows in the following examples: <pre><code># Show entire tree (excluding default dirs)\npython -m app.utils.tree\n\n# Show specific directory\npython -m app.utils.tree app/services\n\n# Show only 1 level deep\npython -m app.utils.tree . 1\n</code></pre></p>"},{"location":"user-guide/utils/#logging_examplespy","title":"<code>logging_examples.py</code>","text":"<p>This code is essentially a test script to test the logging functionality and configuration and will populate test log files in the following directory </p> <p><pre><code>nanobot-poc/\n\u251c\u2500\u2500 logs/\n</code></pre> It is fun from the terminal when in the root <code>nanobot-poc</code> directory with the following command <pre><code>python -m app.utils.logging_examples\n</code></pre></p> <p>I think eventually we may want to move this to test functionality</p>"},{"location":"user-guide/vector-search/","title":"Vector Search","text":"<p>This guide explains how to use NanoBot's vector search capabilities to find relevant information in your processed documents.</p>"},{"location":"user-guide/vector-search/#understanding-vector-search","title":"Understanding Vector Search","text":"<p>NanoBot uses vector similarity search to find information in your documents. Unlike traditional keyword search, vector search understands the semantic meaning of your query, allowing it to find relevant information even when the exact words don't match.</p>"},{"location":"user-guide/vector-search/#how-it-works","title":"How It Works","text":"<ol> <li>Your query is converted to a vector embedding using the same model used for document chunks</li> <li>The database finds chunks with vectors most similar to your query vector</li> <li>Results are ranked by similarity score (higher is better)</li> <li>Optional filters can narrow down results by document or chunking strategy</li> </ol>"},{"location":"user-guide/vector-search/#basic-search","title":"Basic Search","text":"<p>The simplest way to search is to provide a query text:</p> <pre><code>from app.database.transaction import transaction\nfrom app.database.db_retrieval import search_similar_chunks\n\nwith transaction() as conn:\n    results = search_similar_chunks(\n        conn=conn,\n        query_text=\"What is machine learning?\",\n        limit=5  # Return top 5 results\n    )\n\n    # Display results\n    for result in results:\n        print(f\"Similarity: {result['similarity']:.4f}\")\n        print(f\"Text: {result['text'][:200]}...\")\n        print(\"---\")\n</code></pre>"},{"location":"user-guide/vector-search/#filtered-search","title":"Filtered Search","text":"<p>You can narrow down search results using filters:</p> <pre><code>from app.database.transaction import transaction\nfrom app.database.db_retrieval import search_similar_chunks_with_filters\n\nwith transaction() as conn:\n    results = search_similar_chunks_with_filters(\n        conn=conn,\n        query_text=\"What is machine learning?\",\n        limit=5,\n        chunking_strategy=\"balanced\",  # Filter by chunking strategy\n        filename=\"machine_learning.pdf\"  # Filter by filename\n    )\n</code></pre>"},{"location":"user-guide/vector-search/#available-filters","title":"Available Filters","text":"<ul> <li>chunking_strategy: Filter by the chunking strategy used when processing the document</li> <li>filename: Filter by the source document filename</li> </ul>"},{"location":"user-guide/vector-search/#getting-metadata","title":"Getting Metadata","text":"<p>To help with filtering, you can retrieve available metadata values:</p> <pre><code>from app.database.transaction import transaction\nfrom app.database.db_retrieval import get_chunking_strategies, get_filenames\n\nwith transaction() as conn:\n    # Get available chunking strategies\n    strategies = get_chunking_strategies(conn)\n    print(f\"Available strategies: {strategies}\")\n\n    # Get available filenames\n    filenames = get_filenames(conn)\n    print(f\"Available files: {filenames}\")\n</code></pre>"},{"location":"user-guide/vector-search/#understanding-search-results","title":"Understanding Search Results","text":"<p>Each search result contains:</p> <ul> <li>text: The content of the chunk</li> <li>similarity: A score between 0 and 1 indicating how similar the chunk is to your query (higher is better)</li> <li>metadata: Additional information about the chunk, including:</li> <li>filename: The source document</li> <li>page_numbers: The pages where this chunk appears</li> <li>title: The document title or section heading</li> <li>headings: List of headings associated with this chunk</li> <li>chunking_strategy: The strategy used to create this chunk</li> </ul>"},{"location":"user-guide/vector-search/#using-the-streamlit-interface","title":"Using the Streamlit Interface","text":"<p>The Streamlit interface provides a user-friendly way to search your documents:</p> <ol> <li> <p>Start the Streamlit app:    <pre><code>streamlit run nanobot_poc.py\n</code></pre></p> </li> <li> <p>Enter your query in the search box</p> </li> <li>Use the sidebar to configure search parameters:</li> <li>Number of chunks to retrieve</li> <li>Chunking strategy filter</li> <li>Source document filter</li> <li>Click \"Search\" to execute the query</li> <li>View the results, which include:</li> <li>Chunk text</li> <li>Similarity score</li> <li>Source document and page numbers</li> <li>Other metadata</li> </ol>"},{"location":"user-guide/vector-search/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/vector-search/#combining-with-openai","title":"Combining with OpenAI","text":"<p>You can use the retrieved chunks as context for OpenAI to generate more comprehensive answers:</p> <pre><code>from app.services.openai_service import get_chat_response\nfrom app.database.transaction import transaction\nfrom app.database.db_retrieval import search_similar_chunks_with_filters\n\nwith transaction() as conn:\n    # Search for relevant chunks\n    chunks = search_similar_chunks_with_filters(\n        conn=conn,\n        query_text=\"What is machine learning?\",\n        limit=5\n    )\n\n    # Use chunks as context for OpenAI\n    response = get_chat_response(\n        prompt=\"What is machine learning?\",\n        context_chunks=chunks\n    )\n\n    print(response)\n</code></pre>"},{"location":"user-guide/vector-search/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Limit: Adjust the <code>limit</code> parameter based on your needs. Higher values return more results but may include less relevant chunks.</li> <li>Filters: Use filters to narrow down results when you know which documents or chunking strategies are most relevant.</li> <li>Query Formulation: Be specific in your queries for better results. Vector search works best with clear, focused questions.</li> </ul>"},{"location":"user-guide/vector-search/#best-practices","title":"Best Practices","text":"<ul> <li>Start broad, then narrow: Begin with unfiltered searches, then add filters if needed</li> <li>Experiment with chunking strategies: Different strategies work better for different types of queries</li> <li>Use natural language: Phrase queries as you would ask a human</li> <li>Provide context: Longer, more detailed queries often yield better results</li> <li>Review metadata: Check the source document and page numbers to understand where information comes from</li> </ul>"}]}