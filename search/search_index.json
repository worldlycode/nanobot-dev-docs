{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NanoBot-Dev Documentation \ud83e\udd16","text":"<p>Welcome to the NanoBot-Dev documentation. This site provides comprehensive information about using and extending the NanoBot development pipeline.</p>"},{"location":"#what-is-nanobot","title":"What is NanoBot?","text":"<p>NanoBot is a document processing and retrieval system with vector search capabilities. It allows you to:</p> <ul> <li>Process documents (PDF, DOCX, TXT, Google Sheets etc)</li> <li>Chunk documents into manageable pieces</li> <li>Generate vector embeddings</li> <li>Store the document data and text in a relational database</li> <li>Search documents via a simple chatbot interface</li> </ul> <p></p>"},{"location":"#purpose-of-this-development-codebase","title":"Purpose of this development codebase","text":"<p>We are working here to develop a codebase that creates an intuitive pipeline that can be more easily followed, understood and modified by all members involved in the development. The Dev component is a robust pipeline that can empower people and orginazitions to generate their very own Bot with their own documentation, enabling them to also demonstrate and understand the power and potential of GenAI in their own labs, facilities and research.</p> <p>This project presumes that an out-of-the box chatbot developed for you using many onlne WYSIWYG frameworks, is great for concept, but often fails on utility.  By introducing and teaching these concepts in a python codebase framework, we open up the power of Generative AI with only a few custimizations.  </p> <p>It is our hope by doing so, that we not only create a cohesive codebase with functional code for our respective facilities, but that we are able to bring new menbers on board to help them be able to emply instances of this codebase at their own locations, and that they are also able to contribute themselves in building out this worthwhile effort.  </p> <p>This building of Code simultaneously builds community, and further collaboration -- this is \\(\\mathbf{C}^3\\)</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Getting Started - Installation and basic setup</li> <li>User Guide - Detailed usage instructions</li> <li>API Reference - Technical reference for developers</li> <li>Examples - Code examples and tutorials</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>this is where examples will go</p>"},{"location":"getting-started/","title":"Getting Started with NanoBot","text":"<p>This guide will help you set up and start using NanoBot.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>PostgreSQL with pgvector extension</li> <li>OpenAI API key</li> </ul>"},{"location":"getting-started/#setup","title":"Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/worldlycode/nanobot-dev.git\ncd nanobot-dev\n</code></pre></p> </li> <li> <p>Create a virtual environment:    <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Set up environment variables:    Create a <code>.env</code> file with the following variables:    <pre><code>OPENAI_API_KEY=your_openai_api_key\nLOCAL_DB_NAME=nanobot\nLOCAL_DB_USER=postgres\nLOCAL_DB_PASSWORD=your_password\nLOCAL_DB_HOST=localhost\nLOCAL_DB_PORT=5432\nLOGFIRE_TOKEN=your_logfire_token\nNEO\n</code></pre></p> </li> <li> <p>Initialize the database:    <pre><code>python -m app.database.db_setup\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#running-the-application","title":"Running the Application","text":"<p>Start the Streamlit web interface:</p> <pre><code>streamlit run nanobot_poc.py\n</code></pre> <p>This will open a web browser with the NanoBot interface.</p>"},{"location":"api/database/","title":"Database","text":""},{"location":"api/services/","title":"Services","text":""},{"location":"blog/","title":"Hello World \ud83c\udf10","text":"<p>Welcome to the NanoBot-POC development blog! Here we share updates, insights, and lessons learned while building this project.  </p> <p>Click through the topics in the left sidebar to browse the selection.  </p>"},{"location":"blog/00-blog-template/","title":"[Your Post Title Here]","text":"<p>To copy this template: <pre><code>   cp docs/blog/blog-template.md docs/blog/YYYY-MM-DD-title.md\n</code></pre></p> <p>Date: [Month Day, Year] Author: Worldly Woman</p> <pre><code>\"hello world \ud83c\udf10\"</code></pre> <p>[Write your opening paragraph here. Introduce the topic and why it matters.]</p>"},{"location":"blog/00-blog-template/#main-section-title","title":"[Main Section Title]","text":""},{"location":"blog/00-blog-template/#subsection-title","title":"[Subsection Title]","text":"<p>[Main content goes here. You can include links like this: Link Text]</p> <p>You can include images like this:   </p> <p></p> <p>or like this:</p> <p>NOTE: [Use blockquotes for important notes or callouts]</p>"},{"location":"blog/00-blog-template/#another-subsection","title":"[Another Subsection]","text":"<p>[More content here. Feel free to include code samples:]</p> <pre><code># Example code\ndef hello_world():\n    print(\"Hello, documentation world!\")\n</code></pre>"},{"location":"blog/00-blog-template/#another-main-section","title":"[Another Main Section]","text":"<p>[Continue with more content as needed]</p>"},{"location":"blog/00-blog-template/#technical-details","title":"[Technical Details]","text":"<p>[If applicable, include technical details, steps, or instructions]</p> <pre><code># Example command\ncommand --option value\n</code></pre>"},{"location":"blog/00-blog-template/#detailed-subsection","title":"[Detailed Subsection]","text":"<p>[Add more detailed information if necessary]</p>"},{"location":"blog/00-blog-template/#important-notes","title":"Important Notes","text":"<ul> <li>[First important point]  </li> <li>[Second important point]  </li> <li>[Third important point]</li> </ul>"},{"location":"blog/00-blog-template/#closing-thoughts","title":"Closing Thoughts","text":"<p>[Summarize what you've covered and include any final thoughts or future directions]</p> <p>[Optional: Add a fun closing remark or personal touch]</p>"},{"location":"blog/docling/","title":"Understanding IBM's Docling","text":"<p>Date: April 25, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>I am trying to understand the Docling Package, as it is powerful but somewhat complicated under the hood.  The first thing I am beginning to understand is that Docling is the open source itteration of a different tool it released in 2022, called the Deep Search for Discovery Toolkit (DS4SD).  Seep Search was the original solution for PDF documents, and now in 2024 they released docling in the Arxiv paper found at this link.  </p>"},{"location":"blog/docling/#pdf-conversions","title":"PDF Conversions","text":"<p>This is perhaps the main reason to use Docling.  Most python package PDF converters handle the text of a PDF document, and possibly even try to identify tables and images. But the latter in my experience they do not do well.  In addition to that the traditional packages do not capture the context, so they are not able to handle the chunking of \"Sections\" properly.  This means they may not know that a paragraph belongs to a particular section, and perhaps should not be spilt etc.  </p> <p>In any event, these are thisngs that Docling does very well, expecially for something that runs locally and free of charge.  (There are models such as Unstructures, Azure Intellisense, etc that also do a reasonable job but cost money).  Lastly there are methods that are rather state of the art that use MMLM with vision capabilities, to take in each PDF page as an image and then using these models to identify the areas and components of each page.  </p> <p>However, with Docling, IBM has trained two models that run locally (either on CPU or with GPU assist) These models are found on Hugging Face and include:</p> <ul> <li> <p>Layout Model: The layout model will take an image from a page and apply RT-DETR model (RT-DETR is an object detection model that stands for \u201cReal-Time DEtection Transformer.\u201d) in order to find different layout components. It currently detects the labels: Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title. As a reference (from the DocLayNet-paper), this is the performance of standard object detection methods on the DocLayNet dataset compared to human evaluation </p> </li> <li> <p>Table Former: The tableformer model will identify the structure of the table, starting from an image of a table. It uses the predicted table regions of the layout model to identify the tables. Tableformer has SOTA table structure identification.  The Arxiv paper can be found at the following link.  </p> </li> </ul> <p>So we have a lot to figure out here.  But I am convinced this is the way to go for an out-of-the-box methodology.  There is still a learning curve to optimize how to use the chunking module to optimize the vector embeddings.  </p>"},{"location":"blog/logging-2025-03-06/","title":"Logging with Python's Standard Library","text":"<p>Date: March 6, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre> <p>Logging, or tracking what happens as your code executes is pretty important to knowing what happened when things don't go as planned.  I have learned it is like inserting print statements in your code, but these statement can print out to the console (terminal or stdout), to a file, to an external logger (such as Logfire, which I will talk about in another post).  </p> <p>The bottom line is that you can write an awful lot of code in a notebook, or in small modules, and not have to track things.  But when things start getting big, with thousands of lines of code, you really need to be able to log what is going on especially to find bugs etc.  </p> <p>Plus, learning about logging makes you feel like a pro, like you have entered the big-time.  So lets get logging.  </p>"},{"location":"blog/logging-2025-03-06/#pythons-logging-module","title":"Python's Logging Module","text":""},{"location":"blog/logging-2025-03-06/#installing","title":"Installing","text":"<p>There is nothing to install, the logging module comes standard with Python!  You just need to add the following at the top of your files. <pre><code>import logging\n</code></pre></p>"},{"location":"blog/logging-2025-03-06/#setup-logging-config","title":"Setup Logging Config","text":"<p>So I started by following a few tutorials.  The one that helped me the most was from mCoding and called Modern Python Logging.  Click on thumbnail below to be redirected to the video.  </p> <p> </p> <p>[Main content goes here. You can include links like this: Link Text]</p> <p>NOTE: [Use blockquotes for important notes or callouts]</p>"},{"location":"blog/logging-2025-03-06/#another-subsection","title":"[Another Subsection]","text":"<p>[More content here. Feel free to include code samples:]</p> <pre><code># Example code\ndef hello_world():\n    print(\"Hello World! \ud83c\udf10\")\n</code></pre> <pre><code>mkdocs serve\n</code></pre>"},{"location":"blog/logging-2025-03-06/#another-main-section","title":"[Another Main Section]","text":"<p>[Continue with more content as needed]</p>"},{"location":"blog/logging-2025-03-06/#technical-details","title":"[Technical Details]","text":"<p>[If applicable, include technical details, steps, or instructions]</p> <pre><code># Example command\ncommand --option value\n</code></pre>"},{"location":"blog/logging-2025-03-06/#detailed-subsection","title":"[Detailed Subsection]","text":"<p>[Add more detailed information if necessary]</p>"},{"location":"blog/logging-2025-03-06/#important-notes","title":"Important Notes","text":"<ul> <li>[First important point]  </li> <li>[Second important point]  </li> <li>[Third important point]</li> </ul>"},{"location":"blog/logging-2025-03-06/#closing-thoughts","title":"Closing Thoughts","text":"<p>[Summarize what you've covered and include any final thoughts or future directions]</p> <p>[Optional: Add a fun closing remark or personal touch]</p>"},{"location":"blog/mkdocs-2025-03-08/","title":"Starting with MkDocs - Love at first use","text":"<p>Date: March 8, 2025 Author: sam-i-am</p>"},{"location":"blog/mkdocs-2025-03-08/#hello-world","title":"Hello World! \ud83c\udf10","text":"<p>I am sitting here in the St Agnus NYPL Library, where I spent a lot of the past week, writing code.  This has been an incredible journey.  It was only 6 days ago, that I took the time to try and understand how to use Cursor, the VSCode AI developer IDE.  [With Cursor] I have written about 3000 lines of working code, developing the pipeline for Nanobot.  </p> <p>I now need to start with documentation, and I chose to start with MkDocs, and it is rather nice!  </p>"},{"location":"blog/mkdocs-2025-03-08/#project-documentation-with-markdown","title":"Project documentation with Markdown","text":""},{"location":"blog/mkdocs-2025-03-08/#what-it-does","title":"What it Does","text":"<p>You can find it at MkDocs Pages, and the Github repo is at Github MkDocs</p> <p>The key is that you can do all of the documentation in markdown, which makes understanding and updating easy.  It also gives you a very nice web interface, serving a site and can also deploy to github.io to make the docs part of your repo.  Mindblowing \ud83e\udd2f.</p>"},{"location":"blog/mkdocs-2025-03-08/#choosing-your-theme","title":"Choosing Your Theme","text":"<p>The first thing to do is to choose your theme.  There are two built-in themes: * mkdocs * readthedocs</p> <p>There are also many third part themes, which you are advised to use at your own risk.  However, there is another which is highly used and well suported and documented, which has 22k stars and is written by squidfunk:</p> <ul> <li>material</li> </ul> <p>This is what I have decided to use</p>"},{"location":"blog/mkdocs-2025-03-08/#choosing-your-plugins","title":"Choosing Your Plugins","text":"<p>Plugins are sometimes third party items.  Be careful.  I tried the mkdocs-video and it did weird stuff and I uninstalled it.  In any event there is a list of Themes and Plugins at the MkDocs Catalog in the README.  </p>"},{"location":"blog/mkdocs-2025-03-08/#how-to-install-it","title":"How to install it","text":"<p>There were only 3 pip installs that I had to do.  the <code>mkdocstrings[python]</code> will evidently read your docstrings and create the API docs from them, but I have not used this feature yet so will update when I get there.  </p> <pre><code># Core MkDocs package\npip install mkdocs\n\n# Material theme for MkDocs\npip install mkdocs-material\n\n# MkDocstrings for API documentation from docstrings\npip install mkdocstrings[python]\n</code></pre>"},{"location":"blog/mkdocs-2025-03-08/#yml-file-structure","title":"YML File Structure","text":"<p>You will also need a YML file with the configuration <code>mkdocs.yml</code>. My current YAML file looks like:</p> <pre><code>site_name: NanoBot Documentation \ud83e\udd16\nsite_description: Documentation for the NanoBot POC project\nsite_author: Your Name\n\n# Standard docs directory\ndocs_dir: docs\n\ntheme:\n  name: material\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n\n    # Dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.instant\n    - toc.integrate\n    - search.suggest\n    - search.highlight\n\nmarkdown_extensions:\n  - pymdownx.highlight\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.tasklist\n  - admonition\n  - toc:\n      permalink: true\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n  - md_in_html\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          selection:\n            docstring_style: google\n          rendering:\n            show_source: true\n            show_root_heading: true\n\nnav:\n  - Home: index.md\n  - Getting Started: getting-started.md\n  - User Guide:\n    - Overview: user-guide/index.md\n    - Document Processing: user-guide/document-processing.md\n    - Vector Search: user-guide/vector-search.md\n  - API Reference:\n    - Database: api/database.md\n    - Services: api/services.md\n  - Examples: examples.md\n  - Blog: \n    - Overview: blog/index.md\n    - MkDocs: blog/mkdocs-2025-03-08.md\n</code></pre>"},{"location":"blog/mkdocs-2025-03-08/#docs-file-structure-holds-all-of-your-markdown-files","title":"docs File structure (Holds all of your Markdown files)","text":"<p>This is my current dir structure.  I place the <code>/docs</code> directory in the root of my nanobot-poc project.  </p> <p>NOTE: The <code>index.md</code> files are necessary</p> <pre><code>docs/\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 database.md\n\u2502   \u2514\u2500\u2500 services.md\n\u251c\u2500\u2500 blog/\n\u2502   \u251c\u2500\u2500 post-2025-03-08.md\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 img/\n\u2502   \u251c\u2500\u2500 nanobot_schema.jpg\n\u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 document-processing.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 vector-search.md\n\u251c\u2500\u2500 examples.md\n\u251c\u2500\u2500 getting-started.md\n\u2514\u2500\u2500 index.md\n</code></pre>"},{"location":"blog/mkdocs-2025-03-08/#how-to-use-it","title":"How to use it","text":"<p>I havent worked with this but more than 1 hour now, but here is what I know:</p> <p>You can serve the pages locally by running the following command from the root <pre><code>mkdocs serve\n</code></pre> This will launch a local server on port 8000 <code>http://127.0.0.1:8000/</code> where you interact with the HTML version of it.  It is evidently a Bootstrap implementation.  </p> <p>NOTE: If you change the YML file you will need to stop the server <code>ctl C</code> and then restart it again by <code>mkdocs serve</code></p> <p>You can add documents to this but to get them seem in the navigation sidebar you need to add them to the YML file.  If you know the file name you can still navigate to it using the URL but you will not see it in the <code>nav</code> sidebar.  </p> <p>You can deploy this to github.  In the case of my own repo of <code>nanobot-poc</code> I would navigate to <code>srobertsphd.github.io/nanobot-poc</code> and this is where it would reside.  There is an order of events though. </p>"},{"location":"blog/mkdocs-2025-03-08/#updating-github-repo","title":"Updating GitHub repo","text":"<ol> <li>Update your Markdown Docs</li> <li>Update you <code>mkdocs.yml</code> file to unclude any new docs</li> <li>Navigate to your project directory</li> <li>Push the updated mkdown to the repo <pre><code>git add docs/ mkdocs.yml\ngit commit -m \"updated documentation\"\ngit push\n</code></pre></li> <li>Deploy to GitHub Pages using MkDocs  </li> </ol> <pre><code>  mkdocs gh-deploy\n</code></pre>"},{"location":"blog/mkdocs-2025-03-08/#important-notes","title":"Important Notes","text":"<ul> <li>If your repository is private, you'll need a GitHub Pro, Team, or Enterprise account to use GitHub Pages  </li> <li>It may take a few minutes for your site to be available after deployment  </li> <li>You can check the status of your GitHub Pages deployment in your repository settings</li> </ul>"},{"location":"blog/mkdocs-2025-03-08/#content-tabs","title":"Content tabs","text":"<p>The indenting here really matters -- needs to be double tabbed</p> <p>this is the markdown for the three tabs below</p> <pre><code>=== \"plain Text\"\n\n    This is some plain text with code\n\n    ```python\n    print(\"hello world\")\n    ```\n\n=== \"Unordered List\"\n\n    * first\n    * second\n    * third\n\n=== \"Ordered List\"\n\n    1. first\n    2. second\n    3. third\n</code></pre> <p>And this renders as the three tabs below</p> plain TextUnordered ListOrdered List <p>This is some plain text with code</p> <pre><code>print(\"hello world\")\n</code></pre> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <ol> <li>first</li> <li>second</li> <li>third</li> </ol>"},{"location":"blog/mkdocs-2025-03-08/#admonishions","title":"Admonishions","text":"<p>Examople of an admonition/callout with a title:</p> <p>Note the types here -- there is also * note * info * success * tip</p> <p>Each have their own icons.  check it out!</p> <p>Title of the Callout</p> <p>this is some text that is indented with 2 tabs.  here is some more text.  also i wanted to say hello world.  this is an amazing place to be these days</p> Title of the COLLAPSIBLE Callout [CLICK on me] <p>this is some text that is indented with 2 tabs.  here is some more text.  also i wanted to say hello world.  this is an amazing place to be these days</p>"},{"location":"blog/mkdocs-2025-03-08/#diagram-examples","title":"Diagram Examples","text":""},{"location":"blog/mkdocs-2025-03-08/#flowcharts","title":"Flowcharts","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Failure?};\n  B --&gt;|Yes| C[Investigate...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Success!];</code></pre>"},{"location":"blog/mkdocs-2025-03-08/#adding-mathjax-for-equation-formatting-and-display","title":"Adding MathJax for Equation formatting and Display","text":"<p>Taken from the documents -- you will need to add a javascripts folder with the following code, and also modify the mkdocs.yml file as below:</p> docs/javascript/mathjax.jsmkdocs.yml <pre><code>window.MathJax = {\n  tex: {\n    inlineMath: [[\"\\\\(\", \"\\\\)\"]],\n    displayMath: [[\"\\\\[\", \"\\\\]\"]],\n    processEscapes: true,\n    processEnvironments: true\n  },\n  options: {\n    ignoreHtmlClass: \".*|\",\n    processHtmlClass: \"arithmatex\"\n  }\n};\n\ndocument$.subscribe(() =&gt; { \n  MathJax.startup.output.clearCache()\n  MathJax.typesetClear()\n  MathJax.texReset()\n  MathJax.typesetPromise()\n})\n</code></pre> <pre><code>markdown_extensions:\n  - pymdownx.arithmatex:\n      generic: true\n\nextra_javascript:\n  - javascripts/mathjax.js\n  - https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js\n</code></pre>"},{"location":"blog/mkdocs-2025-03-08/#closing-comments","title":"Closing Comments","text":"<p>Writing code is so much fun.  Learning new libraries in minutes is amazing fun.  Leaning how to document things and start writing in less than an hour and having it look good and be accessible --&gt; INCREDIBLE good feeling.   </p> <p>This is a pretty sweet gig.  </p>"},{"location":"blog/mkdocs-2025-03-08/#references","title":"References","text":"<ol> <li> <p>James Willet Dev Full Tutorial To Build And Deploy Your Docs Portal  https://www.youtube.com/watch?v=xlABhbnNrfI\u00a0\u21a9</p> </li> <li> <p>James Willet Dev Getting Started with Material for MkDocs -- https://jameswillett.dev/getting-started-with-material-for-mkdocs/ \u21a9</p> </li> </ol>"},{"location":"blog/packages-and-imports/","title":"Managing Packages and Imports","text":"<p>Date: April 25, 2025 Author: sam-i-am</p> <pre><code>\"Hello World! \ud83c\udf10\"</code></pre>"},{"location":"blog/packages-and-imports/#cleaning-up-your-virrual-environment","title":"Cleaning up your virrual environment","text":"<p>One always wants to would in a development environment.  most often this is done by creating an environment that your codebase depends on using something like <code>venv</code>, which is what we are currently using in our projects.  </p> <p>One of the issues is that every time you do a <code>pip install [package name]</code> it will also install all of the dependencies that that package depends on.  However, in the requirements.txt file it is best practice to only list the top level dependencies, and let pip resolve and install all of the subdependencies.  </p> <p>If you are like me and have a requirements files that includes all of the dependencies including the sub-dependencies, and need to clean all of that up, you can do the following:</p> <ol> <li>Create a backup of your current requirements.txt file <pre><code>cp requirements.txt requirements.txt.bak\n</code></pre></li> <li>Install pipreqs in your activated virtual environment <pre><code>pip install pipreqs\n</code></pre></li> <li>Run pipreqs from the project root <pre><code>pipreqs . --force\n</code></pre></li> </ol>"},{"location":"blog/packages-and-imports/#importing-and-using-local-packages","title":"Importing and using local packages","text":"<p>his is a good reference for importing packages:</p> <p>https://www.youtube.com/watch?v=VEbuZox5qC4</p> <p>Tech with Tim</p> <p>from the chunking.py file in the document_conversion folder, I want to import the Chunks and ChunkMetadata classes from the models.db_schemas module.</p> <pre><code>import sys\nimport os\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(parent_dir)\nfrom models.db_schemas import Chunks, ChunkMetadata\n</code></pre> <ol> <li>os.path.abspath(file): This gets the absolute path of the current file (chunking.py).</li> <li>os.path.dirname(): Called twice to move up two directory levels, reaching the root of your project (nanobot-poc).</li> <li>sys.path.append(parent_dir): Adds the parent directory to sys.path, which is the list of directories Python searches for modules. By including the parent directory, Python can now find models.db_schemas because models is a sibling directory to document_conversion.</li> <li>Import Statement: With the parent directory in sys.path, the import from models.db_schemas import Chunks, ChunkMetadata works because Python can now locate the models directory.</li> </ol> <p>This method effectively adjusts the module search path at runtime, allowing you to import modules from sibling directories.</p>"},{"location":"blog/pgadmin/","title":"configurating PgAdmin with a local or dockerized postgres database","text":"<ol> <li>install pgadmin4 on windows</li> <li>run pgadmin4</li> <li>right click on \"servers\" and click \"register\" -&gt; \"server\"</li> <li>enter the following information:<ul> <li>General:<ul> <li>Name: {name of the instance of the database}</li> </ul> </li> <li>Connection:<ul> <li>Host: {host of the database} -- if local it will be 127.0.0.1</li> <li>Port: {port of the database} -- default is 6432, but this is often already used, so could create a new port number</li> <li>Maintenance database: {name of the database} -- if local it will be postgres</li> <li>Username: {username of the database} -- if local it will be postgres</li> <li>Password: {password of the database} -- if local it will be postgres</li> </ul> </li> <li>Save</li> </ul> </li> </ol> <p>At this point the connect will either be made or if there is an error you should follow the errors to fix the issue.  </p>"},{"location":"blog/scripts-and-modules/","title":"Scripts and modules","text":""},{"location":"blog/scripts-and-modules/#option-1-run-a-module-as-the-main-program","title":"Option 1:  run a module as the main program","text":"<ul> <li>remember tha a file contining python code used by other parts of a package is a \"module\" even if it only has one line of code in it</li> <li>The code must be in the directory tree of the projects and this must be executed from the project root directory</li> <li>this will execute all of the code in that module, and is a good way to execute <code>__main__</code> code</li> </ul> <p>To execute this you want to use the following syntax <pre><code>python -m modulepath.modulename\n</code></pre></p> <ul> <li>the <code>-m</code> means that it is searching for a module</li> <li>we give dots - <code>.</code> notatiion as this is how we define the paths as in an import statement.  </li> <li>there is no extension <code>.py</code> as we are refereing to it by its import path, not its filename</li> </ul>"},{"location":"blog/scripts-and-modules/#option-2-execute-a-standalone-script","title":"Option 2: Execute a standalone script","text":"<p><pre><code>python run_example_directly.py\n</code></pre> In this case the code is not part of the package, and the code resides outside of the package.  </p>"},{"location":"user-guide/","title":"Overview","text":"<p>Welcome to the Nanobot-POC documentation User Guide Pages</p>"},{"location":"user-guide/#our-current-selections-for-infrastructure","title":"Our Current Selections for Infrastructure","text":"<p>As of 4/20/2025</p> <p>The current codebase aims to simplify some of the initial setup and modification of the basic steps of an application.  We have some boilerplate code that organized the steps of a RAG (Retrieval Augmented Generation) Generative AI system, and that has consolidated areas of code where one can modify the parameters of each module, simplyfying the use of the codebase.</p>"},{"location":"user-guide/#python-as-our-language","title":"Python as our Language","text":"<p>Pyuthon is the standard language for most Generative AI applications, and is particullary easy to use. This module require <code>Python 3.12</code> or above.  </p>"},{"location":"user-guide/#document-parsing-and-chunking","title":"Document parsing and chunking","text":"<p>We are currently using IBM's Docling which can be used to convert most general document formats (and even websites) into a format readable and usable to most Generative AI models.  In particular it is especially good at PDFs, and creates a Docling Document that seperatly identifies and extracts text, tables, pictures, document hierarchy with headers, sections and groups, and understands the content within the context of the page.  </p> <p>To start, we are extracting all the text and table and the provenance information (such as pages number, file name, and heading the information is part of)</p>"},{"location":"user-guide/#database","title":"Database","text":"<p>We currently are using Neon.tech for our Postgres database.  Postgres is an optimal solution at this early stage as postgres allows, via the pgvector plugin, semantic similarity searches by vector arithmetic.  </p> <p>This choice to use Neon.Tech provides a web based solution, using the power of postgres and takes advantage of Neon's capability for data versioning and branching.</p>"},{"location":"user-guide/#prompts-with-jinja-templates","title":"Prompts with Jinja Templates","text":"<p>In this structure, we separate out the prompts for each call to the GenAI model so as to extract this from the working code, to make it more transparent, and to be able to easily modify the prompt for testing.</p> <p>The prompts are formatted and delivered to the model using Jinja templates, which also allows us to separate the functionality from the other Python code as well.  </p>"},{"location":"user-guide/#choice-of-generative-ai-models","title":"Choice of Generative AI Models","text":"<p>At this point we are using OpenAI for all aspects of our GenAI needs.  This includes using <code>4o</code> for the cleaning and formatting of the documents, the embedding of the vectors using , and <code>4o</code> for the chat completion steps.  </p> <p>This choice is currently determined by the quality and output of the model, and the ease of interacting with the API.  </p>"},{"location":"user-guide/document-processing/","title":"Document Processing","text":"<p>This guide explains how NanoBot processes documents, from initial conversion to chunking and embedding.</p>"},{"location":"user-guide/document-processing/#supported-document-types","title":"Supported Document Types","text":"<p>NanoBot supports various document types through its document conversion pipeline:</p> <ul> <li>PDF (.pdf): Full support for text extraction, including tables and basic formatting</li> <li>Microsoft Word (.docx): Support for text, tables, and document structure</li> <li>Text files (.txt): Plain text processing</li> <li>Web Pages (URLs): Extract content directly from web pages</li> </ul>"},{"location":"user-guide/document-processing/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<p>The document processing pipeline consists of three main stages:</p> <ol> <li>Document Conversion: Converting documents to a structured format</li> <li>Chunking: Breaking documents into manageable pieces</li> <li>Embedding: Generating vector representations for each chunk</li> </ol>"},{"location":"user-guide/document-processing/#document-conversion","title":"Document Conversion","text":"<p>The first step in processing a document is converting it to a structured format that preserves the document's content and structure. This is handled by the <code>DocumentService</code> class using the Docling library.</p> <pre><code>from app.services.document_service import DocumentService\n\n# Initialize the service\ndocument_service = DocumentService()\n\n# Convert a document\nconverted_doc = document_service.convert_document(\"path/to/document.pdf\")\n</code></pre> <p>During conversion, NanoBot: - Extracts text content - Preserves document structure (headings, paragraphs) - Identifies metadata (title, author, etc.) - Optionally saves intermediate formats for debugging</p>"},{"location":"user-guide/document-processing/#chunking-strategies","title":"Chunking Strategies","text":"<p>After conversion, documents are split into chunks using one of several chunking strategies. Each strategy is optimized for different use cases:</p> Strategy Description Best For default Standard chunking with moderate chunk size General purpose use balanced Balanced approach between context preservation and chunk size Most document types fine_grained Smaller chunks for more precise retrieval Technical documents, reference materials context Larger chunks that preserve more context Q&amp;A, summarization tasks hierarchical Chunks based on document's natural hierarchy Structured documents with clear sections <p>The chunking process is handled by the <code>ChunkingService</code> class:</p> <pre><code>from app.services.chunking_service import ChunkingService\n\n# Initialize the service\nchunking_service = ChunkingService()\n\n# Get information about available strategies\nstrategies = chunking_service.get_available_strategies()\n\n# Chunk a document using a specific strategy\nchunks = chunking_service.chunk_document(document, strategy=\"balanced\")\n\n# Process chunks to extract metadata\nprocessed_chunks = chunking_service.process_chunks(chunks, chunking_strategy=\"balanced\")\n</code></pre>"},{"location":"user-guide/document-processing/#embedding-generation","title":"Embedding Generation","text":"<p>The final step is generating vector embeddings for each chunk using OpenAI's embedding models. These embeddings capture the semantic meaning of the text, enabling similarity search.</p> <pre><code># Generate embeddings for chunks\nchunks_with_embeddings = document_service.get_embeddings_for_chunks(processed_chunks)\n</code></pre> <p>NanoBot uses OpenAI's text embedding models to generate high-dimensional vector representations of each chunk.</p>"},{"location":"user-guide/document-processing/#complete-processing-example","title":"Complete Processing Example","text":"<p>Here's a complete example of processing a document from start to finish:</p> <pre><code>from app.database.transaction import transaction\nfrom app.services.document_service import DocumentService\n\n# Initialize the service\ndocument_service = DocumentService()\n\n# Process a document with the complete pipeline\nwith transaction() as conn:\n    chunks_with_embeddings = document_service.process_document(\n        doc_path=\"path/to/document.pdf\",\n        chunking_strategy=\"balanced\",\n        save_intermediate=True\n    )\n\n    # Now you can insert these chunks into the database\n    # (See the database documentation for details)\n</code></pre>"},{"location":"user-guide/document-processing/#using-the-streamlit-interface","title":"Using the Streamlit Interface","text":"<p>For a more user-friendly experience, you can use the Streamlit interface to process documents:</p> <ol> <li> <p>Start the Streamlit app:    <pre><code>streamlit run nanobot_poc.py\n</code></pre></p> </li> <li> <p>Navigate to the \"Upload\" section</p> </li> <li>Upload your document</li> <li>Select a chunking strategy from the dropdown</li> <li>Click \"Process Document\"</li> </ol> <p>The interface will show progress as the document is processed and provide feedback on the number of chunks created.</p>"},{"location":"user-guide/document-processing/#best-practices","title":"Best Practices","text":"<ul> <li>Choose the right chunking strategy for your document type and use case</li> <li>Process similar documents with the same strategy for consistent results</li> <li>Use meaningful filenames to help with filtering during retrieval</li> <li>Consider document structure when choosing a chunking strategy</li> <li>Monitor token usage when processing large documents</li> </ul>"},{"location":"user-guide/document_structure/","title":"Document structure","text":"<p>``` Lab Documentation \u251c\u2500\u2500 Tools \u2502   \u251c\u2500\u2500 [Tool Name 1] \u2502   \u2502   \u251c\u2500\u2500 Manuals \u2502   \u2502   \u2502   \u2514\u2500\u2500 Submanuals \u2502   \u2502   \u251c\u2500\u2500 Staff SOPs \u2502   \u2502   \u251c\u2500\u2500 User SOPs \u2502   \u2502   \u2514\u2500\u2500 Processes \u2502   \u251c\u2500\u2500 [Tool Name 2] \u2502   \u2502   \u251c\u2500\u2500 Manuals \u2502   \u2502   \u251c\u2500\u2500 Staff SOPs \u2502   \u2502   \u251c\u2500\u2500 User SOPs \u2502   \u2502   \u2514\u2500\u2500 Processes \u2502   \u2514\u2500\u2500 [Tool Name 3] \u2502       \u251c\u2500\u2500 Manuals \u2502       \u251c\u2500\u2500 Staff SOPs \u2502       \u2514\u2500\u2500 Processes \u251c\u2500\u2500 Facilities \u2502   \u251c\u2500\u2500 Manuals \u2502   \u2502   \u2514\u2500\u2500 Submanuals \u2502   \u2514\u2500\u2500 Facility SOPs \u251c\u2500\u2500 General Lab \u2502   \u251c\u2500\u2500 Policies \u2502   \u251c\u2500\u2500 Safety \u2502   \u2514\u2500\u2500 Administration \u2514\u2500\u2500 Chemical     \u251c\u2500\u2500 Safety     \u251c\u2500\u2500 Processes     \u2514\u2500\u2500 Chemical-Specific SOPs</p>"},{"location":"user-guide/git/","title":"Git Collaboration","text":"<p>This is the first time I will be working with other in the same codebase, and thinking about how this actually will work.  This document is a space for us to update how this works best for us.  </p>"},{"location":"user-guide/git/#branches","title":"Branches","text":"<p>Currently cleaning up <code>clean-main</code> but will probably rename as <code>development</code></p>"},{"location":"user-guide/git/#how-to-work-on-your-own-version-of-the-code","title":"How to work on your own version of the code","text":"<p>There are various options  * create your own personal branch -- create a pull request in the development branch when your feature is tested * create a seperate directory called <code>/sandbox</code>.  Make certain your <code>.gitignore</code> file contains this folder so that it is not committed back to the repo before you are ready.  </p>"},{"location":"user-guide/tests/","title":"Tests","text":"<p>Last updated 4/26/2025 by sam-i-am</p> <p>The importance of writing test code on an ongoing basis is becomming apparant to me.  This is especially so with the involvement of multiple contributors.  </p> <p>Obviously this test folder should only exist in the development code (as opposed to the deployed production code)</p>"},{"location":"user-guide/tests/#pytest","title":"Pytest","text":"<p>Currently we are using the <code>pytest</code> module for our tests.  As of the date of the writing of this post the test directory contains the following tests:</p> <pre><code>\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_database_retrieval.py\n\u2502   \u251c\u2500\u2500 test_config_settings.py\n\u2502   \u251c\u2500\u2500 test_transaction.py\n\u2502   \u251c\u2500\u2500 test_chunking_service.py\n\u2502   \u251c\u2500\u2500 test_error_handling.py\n\u2502   \u251c\u2500\u2500 test_document_service.py\n\u2502   \u251c\u2500\u2500 test_pydantic_validator.py\n\u2502   \u251c\u2500\u2500 __init__.py\n</code></pre> <p>This entire test code can be run from the root directory by running</p> <pre><code>pytest tests\n</code></pre> <p>To be rigerously honest, I asked Claude to generate most of these tests, though I have modified some of them.  But the idea is for every module that we write we should have some tests that continuously test the functionality of the module, and run this test code frequently, at least daily.  </p> <p>This will let us catch quickly where our code may have inadvertly gotten broken or had undesired effects and allow us to fix things quickly.  </p>"},{"location":"user-guide/utils/","title":"Utils (Utility function) Folder","text":"<p>April 24, 2025</p> <p>In <code>app/utils</code> directory there are several modules which handle some housekeeping tasks.  We describe these below</p>"},{"location":"user-guide/utils/#treepy","title":"<code>tree.py</code>","text":"<p>The <code>tree.py</code> utility draws a tree of the project or directory.  To run this from the root directory command line (from the <code>nanobot-poc</code> directory), entering</p> <pre><code>python -m app.utils.tree/utils\n</code></pre> <p>will print the following structure in the terminal</p> <pre><code>app/\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init.__.py\n\u2502   \u251c\u2500\u2500 json_formatter.py\n\u2502   \u251c\u2500\u2500 logger.py\n\u2502   \u251c\u2500\u2500 logging_examples.py\n\u2502   \u251c\u2500\u2500 tree.py\n\u2502   \u251c\u2500\u2500 file_handling.py\n\u2502   \u251c\u2500\u2500 tokenizer.py\n</code></pre> <p>To print the entire tree structure you would use:</p> <pre><code>python -m app.utils.tree\n</code></pre> <p>You can also add arguments as follows in the following examples: <pre><code># Show entire tree (excluding default dirs)\npython -m app.utils.tree\n\n# Show specific directory\npython -m app.utils.tree app/services\n\n# Show only 1 level deep\npython -m app.utils.tree . 1\n</code></pre></p>"},{"location":"user-guide/utils/#logging_examplespy","title":"<code>logging_examples.py</code>","text":"<p>This code is essentially a test script to test the logging functionality and configuration and will populate test log files in the following directory </p> <p><pre><code>nanobot-poc/\n\u251c\u2500\u2500 logs/\n</code></pre> It is fun from the terminal when in the root <code>nanobot-poc</code> directory with the following command <pre><code>python -m app.utils.logging_examples\n</code></pre></p> <p>I think eventually we may want to move this to test functionality</p>"},{"location":"user-guide/vector-search/","title":"Vector Search","text":"<p>This guide explains how to use NanoBot's vector search capabilities to find relevant information in your processed documents.</p>"},{"location":"user-guide/vector-search/#understanding-vector-search","title":"Understanding Vector Search","text":"<p>NanoBot uses vector similarity search to find information in your documents. Unlike traditional keyword search, vector search understands the semantic meaning of your query, allowing it to find relevant information even when the exact words don't match.</p>"},{"location":"user-guide/vector-search/#how-it-works","title":"How It Works","text":"<ol> <li>Your query is converted to a vector embedding using the same model used for document chunks</li> <li>The database finds chunks with vectors most similar to your query vector</li> <li>Results are ranked by similarity score (higher is better)</li> <li>Optional filters can narrow down results by document or chunking strategy</li> </ol>"},{"location":"user-guide/vector-search/#basic-search","title":"Basic Search","text":"<p>The simplest way to search is to provide a query text:</p> <pre><code>from app.database.transaction import transaction\nfrom app.database.db_retrieval import search_similar_chunks\n\nwith transaction() as conn:\n    results = search_similar_chunks(\n        conn=conn,\n        query_text=\"What is machine learning?\",\n        limit=5  # Return top 5 results\n    )\n\n    # Display results\n    for result in results:\n        print(f\"Similarity: {result['similarity']:.4f}\")\n        print(f\"Text: {result['text'][:200]}...\")\n        print(\"---\")\n</code></pre>"},{"location":"user-guide/vector-search/#filtered-search","title":"Filtered Search","text":"<p>You can narrow down search results using filters:</p> <pre><code>from app.database.transaction import transaction\nfrom app.database.db_retrieval import search_similar_chunks_with_filters\n\nwith transaction() as conn:\n    results = search_similar_chunks_with_filters(\n        conn=conn,\n        query_text=\"What is machine learning?\",\n        limit=5,\n        chunking_strategy=\"balanced\",  # Filter by chunking strategy\n        filename=\"machine_learning.pdf\"  # Filter by filename\n    )\n</code></pre>"},{"location":"user-guide/vector-search/#available-filters","title":"Available Filters","text":"<ul> <li>chunking_strategy: Filter by the chunking strategy used when processing the document</li> <li>filename: Filter by the source document filename</li> </ul>"},{"location":"user-guide/vector-search/#getting-metadata","title":"Getting Metadata","text":"<p>To help with filtering, you can retrieve available metadata values:</p> <pre><code>from app.database.transaction import transaction\nfrom app.database.db_retrieval import get_chunking_strategies, get_filenames\n\nwith transaction() as conn:\n    # Get available chunking strategies\n    strategies = get_chunking_strategies(conn)\n    print(f\"Available strategies: {strategies}\")\n\n    # Get available filenames\n    filenames = get_filenames(conn)\n    print(f\"Available files: {filenames}\")\n</code></pre>"},{"location":"user-guide/vector-search/#understanding-search-results","title":"Understanding Search Results","text":"<p>Each search result contains:</p> <ul> <li>text: The content of the chunk</li> <li>similarity: A score between 0 and 1 indicating how similar the chunk is to your query (higher is better)</li> <li>metadata: Additional information about the chunk, including:</li> <li>filename: The source document</li> <li>page_numbers: The pages where this chunk appears</li> <li>title: The document title or section heading</li> <li>headings: List of headings associated with this chunk</li> <li>chunking_strategy: The strategy used to create this chunk</li> </ul>"},{"location":"user-guide/vector-search/#using-the-streamlit-interface","title":"Using the Streamlit Interface","text":"<p>The Streamlit interface provides a user-friendly way to search your documents:</p> <ol> <li> <p>Start the Streamlit app:    <pre><code>streamlit run nanobot_poc.py\n</code></pre></p> </li> <li> <p>Enter your query in the search box</p> </li> <li>Use the sidebar to configure search parameters:</li> <li>Number of chunks to retrieve</li> <li>Chunking strategy filter</li> <li>Source document filter</li> <li>Click \"Search\" to execute the query</li> <li>View the results, which include:</li> <li>Chunk text</li> <li>Similarity score</li> <li>Source document and page numbers</li> <li>Other metadata</li> </ol>"},{"location":"user-guide/vector-search/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/vector-search/#combining-with-openai","title":"Combining with OpenAI","text":"<p>You can use the retrieved chunks as context for OpenAI to generate more comprehensive answers:</p> <pre><code>from app.services.openai_service import get_chat_response\nfrom app.database.transaction import transaction\nfrom app.database.db_retrieval import search_similar_chunks_with_filters\n\nwith transaction() as conn:\n    # Search for relevant chunks\n    chunks = search_similar_chunks_with_filters(\n        conn=conn,\n        query_text=\"What is machine learning?\",\n        limit=5\n    )\n\n    # Use chunks as context for OpenAI\n    response = get_chat_response(\n        prompt=\"What is machine learning?\",\n        context_chunks=chunks\n    )\n\n    print(response)\n</code></pre>"},{"location":"user-guide/vector-search/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Limit: Adjust the <code>limit</code> parameter based on your needs. Higher values return more results but may include less relevant chunks.</li> <li>Filters: Use filters to narrow down results when you know which documents or chunking strategies are most relevant.</li> <li>Query Formulation: Be specific in your queries for better results. Vector search works best with clear, focused questions.</li> </ul>"},{"location":"user-guide/vector-search/#best-practices","title":"Best Practices","text":"<ul> <li>Start broad, then narrow: Begin with unfiltered searches, then add filters if needed</li> <li>Experiment with chunking strategies: Different strategies work better for different types of queries</li> <li>Use natural language: Phrase queries as you would ask a human</li> <li>Provide context: Longer, more detailed queries often yield better results</li> <li>Review metadata: Check the source document and page numbers to understand where information comes from</li> </ul>"}]}